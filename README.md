# Awesome Robotics Datasets, Benchmarks & Models

Curated list of simulation and real-world datasets for robot learning, plus benchmarks, simulators, and notable models.

## Table of Contents
- [Other Awesome Lists](#other-awesome-lists)
- [Datasets](#datasets)
  - [Simulated](#simulated)
  - [Real-World](#real-world)
  - [Combined (Sim + Real)](#combined-sim--real)
  - [Dataset Collections](#dataset-collections)
- [Benchmarks](#benchmarks)
- [Physics-Based Simulation Frameworks](#physics-based-simulation-frameworks)
- [Models](#models)
- [Papers to include](#papers-to-include)

## Other Awesome Lists
- [Awesome-VLA-Robotics](https://github.com/ksDreamer/Awesome-VLA-Robotics)
- [Awesome-VLA-Papers](https://github.com/Psi-Robot/Awesome-VLA-Papers)
- [Awesome-VLA](https://github.com/Orlando-CS/Awesome-VLA)
- [awesome-ai-robotics](https://github.com/tc-huang/awesome-ai-robotics)
- [Awesome-Robot-Learning](https://github.com/RayYoh/Awesome-Robot-Learning)
- [Awesome-Implicit-NeRF-Robotics](https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Robotics)
- [awesome-robotics-datasets](https://mint-lab.github.io/awesome-robotics-datasets/)
- [awesome-isaac-gym](https://github.com/robotlearning123/awesome-isaac-gym)
- [awesome-robotics](https://github.com/ahundt/awesome-robotics)
- [awesome-robotics papers](https://github.com/ahundt/awesome-robotics/blob/master/papers.md)
- [awesome-robotics-libraries](http://jslee02.github.io/awesome-robotics-libraries/)
- [awesome-human-robot-interaction](https://github.com/Po-Jen/awesome-human-robot-interaction)
- [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning)

---
# Datasets

## Simulated
| NAME                                                                            | YEAR | SIZE                                                                                                                            | MODALITIES                                                                                                                                                                          | ROBOTS                                                                                                                                         | UNI / <br>ORG                                                                                | Links                                                                                                                                                                                                                                                                                                                                               |
| ------------------------------------------------------------------------------- | ---- | ------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| BEHAVIOR-1k <br>(2025 BEHAVIOR Challenge)                                       | 2025 | ğŸ§© 50 tasks<br>full-length household<br><br>ğŸ•¹ï¸ 10,000<br>teleop. demos.                                                        | ğŸ–¼ï¸  RGB, depth, and instance segmentation images<br><br>ğŸ¤– Robot state, action, task information                                                                                   | Galaxea R1 robot                                                                                                                               | Stanford                                                                                     | ğŸŒ[Website](https://behavior.stanford.edu/challenge/index.html) <br>ğŸ“„<br>[Paper](https://arxiv.org/abs/2403.09227) ğŸ’¾<br>[Data](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)                                                                                                                                                  |
| DexMimicGen                                                                     | 2025 | ğŸ•¹ï¸ 21,000+ <br>demos.<br><br>ğŸ§© 9 tasks                                                                                        | ğŸ–¼ï¸ Image <br><br>ğŸ¤– Robot state, action                                                                                                                                            | GR-1 <br>humanoid, bimanual <br>Panda<br>arms with dexterous hands                                                                             | NVIDIA Research,<br>UT Austin, UC San Diego                                                  | ğŸŒ [Website](https://dexmimicgen.github.io/) <br> ğŸ“„<br> [Paper](https://arxiv.org/abs/2410.24185) <br> ğŸ’¾<br> [Data](https://huggingface.co/datasets/MimicGen/dexmimicgen_datasets/tree/main)                                                                                                                                                      |
| Lerobot (Meta-World+)                                                           | 2025 | ğŸ§© 50 manip.<br>tasks                                                                                                           | ğŸ–¼ï¸ RGB-D image<br><br>ğŸ¤– Robot action,  Environment state                                                                                                                          | Sawyer                                                                                                                                         | Farma foundation, LeRobot                                                                    | ğŸŒ [Website](https://meta-world.github.io/) ğŸ’¾ [Data](https://huggingface.co/datasets/lerobot/metaworld_mt50)                                                                                                                                                                                                                                       |
| ManiSkill 3 (Datasets creation inprogress as of 1.9.2025)                       | 2024 | ğŸ  16 envs.                                                                                                                     | ğŸ¤– demos. (motion planning, teleop) <br> <br> ğŸ–¼ï¸ RGB-D, point clouds                                                                                                               | Unitree <br>G1/H1,<br>Panda, <br>UR10e,<br>Allegro<br>Hand, <br>XArm, <br>[etc.](https://maniskill.readthedocs.io/en/latest/robots/index.html) | UC San Diego <br>(SU Lab)                                                                    | ğŸŒ [Website](https://www.maniskill.ai/) ğŸ“„ [Paper](https://arxiv.org/abs/2410.00425)  ğŸ’¾ [Data](https://huggingface.co/datasets/haosulab/ManiSkill_Demonstrations)                                                                                                                                                                                  |
| LIBERO                                                                          | 2023 | ğŸ§© 130 tasks                                                                                                                    | ğŸ–¼ï¸ RGB images from workspace and wrist cameras <br><br>ğŸ¤– Proprioception <br><br>ğŸ—£ï¸ Language task specifications <br><br>ğŸ“ PDDL scene descriptions                               | Panda                                                                                                                                          | UT Austin, Tsinghua University                                                               | ğŸŒ <br>[Website](https://libero-project.github.io/datasets)<br>ğŸ“„<br>[Paper](https://arxiv.org/abs/2306.03310) <br>ğŸ’¾ <br>[Data](https://libero-project.github.io/datasets)<br>                                                                                                                                                                     |
| Habitat                                                                         | 2023 | Embodied Question Answering,<br>Visual <br>Language Navigation, Rearrange <br>Pick,<br>Object <br>and <br>point goal navigation | ğŸ—£ï¸ Scene description and language annotation (EQA, VLN)                                                                                                                            | Fetch, <br>Virtual <br>Human<br>Agents                                                                                                         | Facebook AI Research, Georgia Tech, <br>UC Berkeley, Simon Fraser University, Intel Research | (2.0)<br>ğŸŒ [Website](https://aihabitat.org/)ğŸ“„ [Paper](https://arxiv.org/abs/2106.14405)<br>(3.0)<br>ğŸŒ [Website](https://aihabitat.org/habitat3/) ğŸ“„ [Paper](https://arxiv.org/abs/2310.13724)  ğŸ’¾ [Data Hugging Face](https://huggingface.co/ai-habitat)<br>ğŸ’¾ [Data GIT](https://github.com/facebookresearch/habitat-lab/blob/main/DATASETS.md) |
| MimicGen                                                                        | 2023 | ğŸ•¹ï¸ 48,000 <br>task demos.<br><br>ğŸ§©12 tasks                                                                                    | ğŸ–¼ï¸ Image<br><br>ğŸ¤– Robot state, action, task observation                                                                                                                           | Panda,<br>Sawyer,<br>IIWA,<br>UR5e                                                                                                             | The University of Texas at Austin                                                            | ğŸŒ [Website](https://mimicgen.github.io/) ğŸ“„ [Paper](https://arxiv.org/abs/2310.17596) ğŸ’¾ [Data](https://huggingface.co/datasets/amandlek/mimicgen_datasets)                                                                                                                                                                                        |
| ManiSkill 2                                                                     | 2023 | ğŸ§© 20 tasks                                                                                                                     | ğŸ¤– demos. (scene and robot states)<br><br>ğŸ–¼ï¸ Replay tools (point cloud, RGB-D)                                                                                                     | Panda                                                                                                                                          | UC San Diego, Tsinghua University                                                            | ğŸŒ [Website](https://maniskill2.github.io/)ğŸ“„ [Paper](https://arxiv.org/abs/2302.04659)ğŸ’¾ [Data](https://huggingface.co/datasets/haosulab/ManiSkill2)                                                                                                                                                                                               |
| CALVIN (**C**omposing **A**ctions from **L**anguage and **Vi**sio**n**) Dataset | 2022 | ğŸ  4 envs.<br>(24 hours of teleoped. â€œplayâ€)                                                                                    | ğŸ–¼ï¸ RGB-D images from both static and gripper-mounted cameras<br><br>ğŸ—£ï¸ Language Annotations<br><br>ğŸ¤– Action (tool center point), State Observation<br><br>ğŸ¤ Tactile image input | Panda                                                                                                                                          | University of Freiburg                                                                       | ğŸŒ [Website](http://calvin.cs.uni-freiburg.de/)  ğŸ“„ [Paper](https://arxiv.org/abs/2112.03227)  ğŸ’¾ [Data](https://github.com/mees/calvin/tree/main/dataset)                                                                                                                                                                                          |
| RLBench (Perceiver-Actor Dataset)                                               | 2022 | Dataset <br>generator                                                                                                           | ğŸ–¼ï¸ Multyple RBG, depth and mask camera views <br><br>ğŸ¤– Episode variation description                                                                                              | Panda                                                                                                                                          | University of Washington, NVIDIA                                                             | ***Perceiver-Actor*** <br>ğŸŒ [Website](https://peract.github.io/)ğŸ“„ [Paper](https://arxiv.org/abs/2209.05451)<br>***RLBench***<br>ğŸŒ [Website](https://sites.google.com/view/rlbench)ğŸ“„ [Paper](https://arxiv.org/abs/1909.12271)                                                                                                                   |
| Mobile Manipulation RoboTurk (MoMaRT)                                           | 2021 | ğŸ•¹ï¸1200 demos. <br><br>ğŸ§© 5 tasks                                                                                               | ğŸ–¼ï¸ Normalized LIDAR scan, RGB, depth<br><br>ğŸ¤– Objects states, robot actions, rewards and propriception                                                                            | Fetch                                                                                                                                          | Stanford University                                                                          | ğŸŒ [Website](https://sites.google.com/view/il-for-mm/home) ğŸ“„ [Paper](https://arxiv.org/abs/2112.05251) ğŸ’¾ [Data](https://sites.google.com/view/il-for-mm/datasets#h.ko0ilbky4y5u)                                                                                                                                                                  |
| BEHAVIOR-100 Dataset                                                            | 2021 | ğŸ•¹ï¸ 500 <br>VR demos.                                                                                                           | ğŸ–¼ï¸ Semantic segmentation, instance segmentation, RGB, depth, highlight<br><br>ğŸ¤–Proprioception, agent action                                                                       | VR agent                                                                                                                                       | Stanford                                                                                     | ğŸŒ [Website](https://behavior.stanford.edu/behavior_100/overview.html)  ğŸ“„ [Paper](https://arxiv.org/abs/2108.03332)  <br>ğŸ’¾ [Data](https://behavior.stanford.edu/behavior_100/dataset.html)                                                                                                                                                        |
| D4RL                                                                            | 2020 | ğŸ§©4 <br>Franka Kitchen with<br>4 subtasks                                                                                       | ğŸ¤– robot and scene state observation                                                                                                                                                | Ant, <br>Hopper, Walker2D, <br>Panda                                                                                                           | Google,<br>UC Berkeley                                                                       | ğŸŒ [Website](https://sites.google.com/view/d4rl-anonymous/) ğŸ“„ [Paper](https://arxiv.org/abs/2004.07219) ğŸ’¾ [Data](https://minari.farama.org/datasets/D4RL/kitchen/)                                                                                                                                                                                |
| ALFRED                                                                          | 2020 | ğŸ•¹ï¸ 8,055 <br>expert demos. <br>(25,743<br>language directives)                                                                 | ğŸ–¼ï¸ Resnet18 features, video sequence<br><br>ğŸ—£ï¸ Language <br><br>ğŸ¤– Action                                                                                                         | AI2-THOR <br>agents                                                                                                                            | University of Washington, Allen Institute for AI                                             | ğŸŒ [Website](https://askforalfred.com/)  ğŸ“„ [Paper](https://arxiv.org/abs/1912.01734)ğŸ’¾ [Data](https://github.com/askforalfred/alfred/tree/master/data)                                                                                                                                                                                             |
| VirtualHome                                                                     | 2018 | ğŸ§©2821<br>Activity <br>Programs,  <br><br>ğŸ§© 5193 Synthetic Programs                                                            | ğŸ—£ï¸ Semantic Segmentation, Natural Language Descriptions <br><br>ğŸ–¼ï¸ RGB-D data<br><br>ğŸ¤– Pose information                                                                          | Virtual <br>avatars                                                                                                                            | MIT, University of Toronto                                                                   | ğŸŒ [Website](http://virtual-home.org/)ğŸ“„ [Paper](http://virtual-home.org/paper/virtualhome.pdf)ğŸ’¾ [Data](https://github.com/xavierpuigf/virtualhome/blob/master/virtualhome/dataset/README.md)                                                                                                                                                      |

<details>
  <summary><i>Additional notes</i></summary>

<table>
  <thead>
    <tr>
      <th>NAME</th>
      <th>DATA FORMAT</th>
      <th>NOTES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BEHAVIOR-1k <br>(2025 BEHAVIOR Challenge)</td>
      <td>LeRobot format (parquet)</td>
      <td>Datasets of human-grounded dexterous bimanual manipulations in house-scale scenes part of the <a href="https://foundation-models-meet-embodied-agents.github.io/eai_challenge/">2025 EAI Challenge</a>.</td>
    </tr>
    <tr>
      <td>DexMimicGen</td>
      <td>HDF5</td>
      <td>Large-scale automated data<br>generation system from small number of human demos. for bi-manual manipulation.</td>
    </tr>
    <tr>
      <td>Lerobot (Meta-World+)</td>
      <td>LeRobot format (parquet)</td>
      <td>Single arm manipulation tasks. Generated using expert policies in the Meta-world simulation benchmarks.</td>
    </tr>
    <tr>
      <td>ManiSkill 3 (Datasets creation inprogress as of 1.9.2025)</td>
      <td>HDF5, JSON</td>
      <td>Single arm manipulation tasks (e.g peg insertion) and some simple humanoid (Unitree G1) and mobile manipulation tasks. In development; most environments are missing demos.</td>
    </tr>
    <tr>
      <td>LIBERO</td>
      <td>HDF5</td>
      <td>Single arm pick and place tasks, human-teleoperated demos. inside simulation.</td>
    </tr>
    <tr>
      <td>Habitat</td>
      <td>JSON</td>
      <td>Habitat 2.0 contains Scans and CAD Replicas (with simulation framework) of interiors. Embodied datasets mainly came with Habitat 3.0 datasets. Relevant Docs for Sim in Habitat 2.0</td>
    </tr>
    <tr>
      <td>MimicGen</td>
      <td>HDF5</td>
      <td>A system for automatically synthesizing large-scale datasets from a small number of human demos. by adapting them to new scene configurations, object instances, and robot arms.</td>
    </tr>
    <tr>
      <td>ManiSkill 2</td>
      <td>HDF5, JSON</td>
      <td>Replaced by ManiSkill 3 (which is still in beta).</td>
    </tr>
    <tr>
      <td>CALVIN (<em>C</em>omposing <em>A</em>ctions from <em>L</em>anguage and <em>Vi</em>sio<span style="font-variant: small-caps;">n</span>) Dataset</td>
      <td>NumPy files</td>
      <td>Single arm manipulation with multiple drawers. Long horizon in terms of sequence length. Data colected with HTC Vive VR headset.</td>
    </tr>
    <tr>
      <td>RLBench (Perceiver-Actor Dataset)</td>
      <td>PNG, Pickle format</td>
      <td>Generated simulation single arm datasets using the RLBench datasets generation utility (using template policies). Data generator example: pregenerated 18 tasks in Perceiver-actor ğŸ’¾ <a href="https://huggingface.co/datasets/hqfang/rlbench-18-tasks">Data</a></td>
    </tr>
    <tr>
      <td>Mobile Manipulation RoboTurk (MoMaRT)</td>
      <td>HDF5</td>
      <td>Simulated mobile manipulation in kitchen environments (e.g setting table from dishwasher, table cleanup to sink).</td>
    </tr>
    <tr>
      <td>BEHAVIOR-100 Dataset</td>
      <td>HDF5</td>
      <td>Datasets of human VR agents interacting with the simulation. First generation of BEHAVIOR replaced by BEHAVIOR 1K</td>
    </tr>
    <tr>
      <td>D4RL</td>
      <td>HDF5</td>
      <td>Franka Kitchen environment<br>containing several common household items: a microwave, a kettle,<br>an overhead light, cabinets, and an oven. The 4 subtasks are: <br>open the microwave, move the kettle, flip the light switch, and slide open the cabinet door,</td>
    </tr>
    <tr>
      <td>ALFRED</td>
      <td>JSON, Raw Images, MP4, PDDL</td>
      <td>Single room interactive visual dataset with high-level goal and low-level<br>natural language instructions for object and environment interaction.</td>
    </tr>
    <tr>
      <td>VirtualHome</td>
      <td>JSON</td>
      <td>Embodied household benchmark, activities in are represented by <em>programs</em> (sequence of actions) and <em>graphs</em> (definition of the environment where the activity takes place).</td>
    </tr>
  </tbody>
</table>
</details>
<details>
<summary><i>Full paper citations </i></summary>
- **BEHAVIOR-1k (2025 BEHAVIOR Challenge)**, BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation, 2022.12, Conference on Robot Learning. [ğŸ“„ Paper](https://arxiv.org/abs/2403.09227) [ğŸ’» Website](https://behavior.stanford.edu/challenge/index.html) [ğŸ’¾ Dataset](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)
    
- **DexMimicGen**, DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning, 2024.10, ICRA 2025. [ğŸ“„ Paper](https://arxiv.org/abs/2410.24185) [ğŸ’» Website](https://dexmimicgen.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/MimicGen/dexmimicgen_datasets/tree/main)
    
- **Lerobot (Meta-World+)**, Meta-World+: An Improved, Standardized, RL Benchmark, 2025.05. [ğŸ“„ Paper](https://arxiv.org/abs/2505.11289) [ğŸ’» Website](https://meta-world.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/lerobot/metaworld_mt50)
    
- **ManiSkill 3**, A Unified Simulator for Physics-Based Robot Learning, 2024.10. [ğŸ“„ Paper](https://arxiv.org/abs/2410.00425) [ğŸ’» Website](https://www.maniskill.ai/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/haosulab/ManiSkill_Demonstrations)
    
- **LIBERO**, LIBERO: Benchmarking Knowledge Transfer in Lifelong Robot Learning, 2023.06. [ğŸ“„ Paper](https://arxiv.org/abs/2306.03310) [ğŸ’» Website](https://libero-project.github.io/datasets) [ğŸ’¾ Dataset](https://libero-project.github.io/datasets)
    
- **Habitat**, Habitat: A Platform for Embodied AI Research, 2019, ICCV. [ğŸ“„ Paper (v2.0)](https://arxiv.org/abs/2106.14405) [ğŸ“„ Paper (v3.0)](https://arxiv.org/abs/2310.13724) [ğŸ’» Website](https://aihabitat.org/) [ğŸ’¾ Dataset](https://huggingface.co/ai-habitat)
    
- **MimicGen**, MimicGen: Language-guided Data Generation for Imitation Learning, 2023.10. [ğŸ“„ Paper](https://arxiv.org/abs/2310.17596) [ğŸ’» Website](https://mimicgen.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/amandlek/mimicgen_datasets)
    
- **ManiSkill 2**, ManiSkill2: A Unified Benchmark for High-Level Robot Manipulation Tasks, 2023.02. [ğŸ“„ Paper](https://arxiv.org/abs/2302.04659) [ğŸ’» Website](https://maniskill2.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/haosulab/ManiSkill2)
    
- **CALVIN Dataset**, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, 2021.12, IEEE Robotics and Automation Letters. [ğŸ“„ Paper](https://arxiv.org/abs/2112.03227) [ğŸ’» Website](http://calvin.cs.uni-freiburg.de/) [ğŸ’¾ Dataset](https://github.com/mees/calvin/tree/main/dataset)
    
- **RLBench (Perceiver-Actor Dataset)**, Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation, 2022.09. [ğŸ“„ Paper](https://arxiv.org/abs/2209.05451) [ğŸ’» Website](https://peract.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/hqfang/rlbench-18-tasks)
    
- **Mobile Manipulation RoboTurk (MoMaRT)**, Learning Human-Guided Mobile Manipulation, 2021.12. [ğŸ“„ Paper](https://arxiv.org/abs/2112.05251) [ğŸ’» Website](https://sites.google.com/view/il-for-mm/home) [ğŸ’¾ Dataset](https://sites.google.com/view/il-for-mm/datasets#h.ko0ilbky4y5u)
    
- **BEHAVIOR-100 Dataset**, BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments, 2021.08, CoRL 2021. [ğŸ“„ Paper](https://arxiv.org/abs/2108.03332) [ğŸ’» Website](https://behavior.stanford.edu/behavior_100/overview.html) [ğŸ’¾ Dataset](https://behavior.stanford.edu/behavior_100/dataset.html)
    
- **D4RL**, D4RL: Datasets for Deep Data-Driven Reinforcement Learning, 2020.04, ICLR 2021. [ğŸ“„ Paper](https://arxiv.org/abs/2004.07219) [ğŸ’» Website](https://sites.google.com/view/d4rl-anonymous/) [ğŸ’¾ Dataset](https://minari.farama.org/datasets/D4RL/kitchen/)
    
- **ALFRED**, ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks, 2019.12. [ğŸ“„ Paper](https://arxiv.org/abs/1912.01734) [ğŸ’» Website](https://askforalfred.com/) [ğŸ’¾ Dataset](https://github.com/askforalfred/alfred/tree/master/data)
    
- **VirtualHome**, VirtualHome: Simulating Household Activities via a Generative and Interactive Model, 2018.06. [ğŸ“„ Paper](http://virtual-home.org/paper/virtualhome.pdf) [ğŸ’» Website](http://virtual-home.org/) [ğŸ’¾ Dataset](https://github.com/xavierpuigf/virtualhome/blob/master/virtualhome/dataset/README.md)
  </details>
---

## Real-world

| NAME            | Links                                                                                                                                                                                                                               | YEAR | SIZE                                                                                | MODALITIES                                                                                                                                                                                                 | ROBOTS                                                                                       | UNIVERSITY / ORG                                                                                                                                                                                              | DATA FORMAT                                               | NOTES                                                                                                                                                                                                                                          |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| DROID           | ğŸŒ [Website](https://droid-dataset.github.io/) ğŸ“„ [Paper](https://arxiv.org/abs/2403.12945) ğŸ’¾ [Data](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing)                    | 2024 | ğŸ  564 scenes<br>ğŸ§© 86 tasks                                                        | ğŸ–¼ï¸ full-HD stereo videos for all three cameras,<br>ğŸ¤– [RLDS](https://droid-dataset.github.io/droid/the-droid-dataset),<br> ğŸ—£ï¸ [Language  annotation (added in 2025)](https://huggingface.co/KarlP/droid) | Franka Panda                                                                                 | 13 Universities (Stanford, Berkeley, Princton, Washington and [others](https://droid-dataset.github.io/))                                                                                                     | RLDS                                                      | One hardware setup across 13 institutions for data collection.                                                                                                                                                                                 |
| Mobile ALOHA    | ğŸŒ [Website](https://mobile-aloha.github.io/) ğŸ“„ [Paper](https://arxiv.org/abs/2401.02117)  ğŸ’¾ [Data](https://drive.google.com/drive/folders/1FP5eakcxQrsHyiWBRDsMRvUfSxeykiDc)                                 | 2024 | ğŸ§© 7 tasks                                                                          | ğŸ–¼ï¸ Three camera positions, cam_high, cam_left_wrist, cam_right_wrist <br>ğŸ¤– action, effort ,  pose <br>                                                                                                   | Mobile base + two ViperX 300 robotic arms. The teleoperation setup uses two WidowX 250 arms. | Stanford University                                                                                                                                                                                           | HDF5                                                      | Bi-manual mobile manipulation. Co-training with existing static ALOHA datasets.                                                                                                                                                                |
| BridgeData V2   | ğŸŒ [Website](https://rail-berkeley.github.io/bridgedata/)  ğŸ“„ [Paper](https://arxiv.org/abs/2308.12952)  ğŸ’¾[Data](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/)                                 | 2023 | ğŸ  24 environments<br>ğŸ§© 13 skills                                                  | ğŸ–¼ï¸ Multiple camera views and depth data<br>ğŸ¤– robot arm trajectory<br> ğŸ—£ï¸ Natural language labels                                                                                                        | WidowX 250                                                                                   | UC Berkeley, Stanford ,Google DeepMind, CMU                                                                                                                                                                   | HDF5                                                      | Teleoperation of the robot with a VR<br>controller.                                                                                                                                                                                            |
| RoboSet         | ğŸŒ [Website](https://robopen.github.io/roboset/) <br> ğŸ“„ [Paper](https://arxiv.org/abs/2309.01918) <br> ğŸ’¾ [Data](https://github.com/vikashplus/robohive/wiki/7.-Datasets)                                      | 2023 | ğŸ•¹ï¸ 30,050 trajectories<br>ğŸ§© 38 tasks                                              | ğŸ–¼ï¸ 4 RGB camera views <br> ğŸ—£ï¸ Language instructions <br> ğŸ¤– Proprioception (state, velocity)                                                                                                             | Franka Emika Panda                                                                           | Carnegie Mellon University                                                                                                                                                                                    | HDF5                                                      | A large-scale, real-world multi-task dataset for everyday household activities in kitchen scenes, collected via kinesthetic and teleoperated demonstrations.                                                                                   |
| Furniture Bench | ğŸŒ [Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) ğŸ“„ [Paper](https://arxiv.org/abs/2305.12821) ğŸ’¾ [Data](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) | 2023 | ğŸ§© 8 different furniture tasks<br>ğŸ•¹ï¸ 5100 teleoperation demos.             | ğŸ–¼ï¸  Wrist camera image, Front camera image <br>ğŸ¤– robot state                                                                                                                                             | Franka Panda                                                                                 | Korea Advanced Institute of Science and Technology                                                                                                                                                            | Pickle format                                             | 3D printed furniture tasks. Data collected with Oculus Quest 2 controller and a keyboard.                                                                                                                                                      |
| RH20T           | ğŸŒ [Website](https://rh20t.github.io/) ğŸ“„ [Paper](https://arxiv.org/abs/2307.00595) ğŸ’¾ [Data](https://rh20t.github.io/#download)                                                                                | 2023 | ğŸ•¹ï¸110,000+ Robot demostrations<br>ğŸ•¹ï¸110,000+ Human demonstrations<br>ğŸ§©140+ tasks | ğŸ–¼ï¸ RGB image, Depth image, Binocular IR images, Audio<br>ğŸ¤– Robot proprioception<br> ğŸ¤ Fingertip tactile<br>  ğŸ—£ï¸ Task language description                                                              | Flexiv, UR5, Franka, Kuka                                                                    | Shanghai AI Laboratory, Beihang University, Shanghai Jiao Tong University, University of Sydney, University of Electronic Science and Technology of China, Nanjing University of Posts and Telecommunications | Compressed tarball of MP4 and NumPy data                  | Includes pairs of human and robot demonstrations for each task. Also contains single sentence task descriptions.                                                                                                                               |
| Dobb-E          | ğŸŒ [Website](https://dobb-e.com/) <br> ğŸ“„ [Paper](https://arxiv.org/abs/2311.16098) <br> ğŸ’¾ [Data](https://github.com/notmahi/dobb-e/)                                                                          | 2023 | ğŸ•¹ï¸ 13 hours of interactions<br>ğŸ  22 homes, ğŸ§©109 tasks                            | ğŸ–¼ï¸ RGB video <br> ğŸ–¼ï¸ Depth video <br> ğŸ¤– Gripper 6D pose, gripper opening                                                                                                                                | Hello Robot Stretch                                                                          | New York University                                                                                                                                                                                           | Custom (includes RGB/depth videos and action annotations) | An open-source framework using a low-cost tool ("The Stick") to collect the "Homes of New York (HoNY)" dataset for teaching robots household tasks via imitation learning.                                                                     |
| MT-Opt          | ğŸŒ [Website](https://karolhausman.github.io/mt-opt/) <br> ğŸ“„ [Paper](https://arxiv.org/abs/2104.08212) <br> ğŸ’¾ [Data](https://www.tensorflow.org/datasets/catalog/mt_opt)                                       | 2021 | ğŸ•¹ï¸ 800,000+ episodes<br>ğŸ§© 12 real-world tasks                                     | ğŸ–¼ï¸ Image <br> ğŸ¤– Actions                                                                                                                                                                                  | Kuka                                                                                         | Robotics at Google                                                                                                                                                                                            | Offline RL dataset format                                 | Multi-robot collective learning system for data collection. Pick Place tasks with 7 Kukas.                                                                                                                                                     |
| RoboNet         | ğŸŒ [Website](https://www.robonet.wiki/) <br> ğŸ“„ [Paper](https://arxiv.org/abs/1910.11215) <br> ğŸ’¾ [Data](https://github.com/SudeepDasari/RoboNet/wiki/Getting-Started)                                          | 2019 | ğŸ•¹ï¸ ~162,000 trajectories                                                           | ğŸ–¼ï¸ Video <br> ğŸ¤– Action sequences, end-effector pose, gripper state                                                                                                                                       | Sawyer, Franka, Kuka, Baxter, WidowX, Fetch, Google Robot                                    | UC Berkeley, Stanford University, University of Pennsylvania, CMU                                                                                                                                             | TFRecord                                                  | Open database for sharing robot experience from 7 different robot platforms and institutions.                                                                                                                                                  |
| BC-Z            | ğŸŒ [Website](https://sites.google.com/view/bc-z/home) <br> ğŸ“„ [Paper](https://arxiv.org/abs/2202.02005) <br> ğŸ’¾ [Data](https://www.kaggle.com/datasets/google/bc-z-robot)                                       | 2021 | ğŸ•¹ï¸ 25,877 demonstrations<br>ğŸ§© 100+ tasks<br>â° 125 robot hours                     | ğŸ–¼ï¸ Vision <br> ğŸ—£ï¸ Language instructions <br> ğŸ¤– Actions                                                                                                                                                  | 12 Google Robots (7-DOF arms)                                                                | Google, UC Berkeley                                                                                                                                                                                           | RLDS (via TFDS)                                           | A large-scale imitation learning system for zero-shot and few-shot generalization to novel manipulation tasks not seen during training. [[7]](https://hyper.ai/en/datasets/35172)[[8]](https://proceedings.mlr.press/v164/jang22a/jang22a.pdf) |

| NAME            | NOTES                                                                                                                                                                                                                                          | DATA FORMAT                                               |
| --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| DROID           | One hardware setup across 13 institutions for data collection.                                                                                                                                                                                 | RLDS                                                      |
| Mobile ALOHA    | Bi-manual mobile manipulation. Co-training with existing static ALOHA datasets.                                                                                                                                                                | HDF5                                                      |
| BridgeData V2   | Teleoperation of the robot with a VR<br>controller.                                                                                                                                                                                            | HDF5                                                      |
| RoboSet         | A large-scale, real-world multi-task dataset for everyday household activities in kitchen scenes, collected via kinesthetic and teleoperated demonstrations.                                                                                   | HDF5                                                      |
| Furniture Bench | 3D printed furniture tasks. Data collected with Oculus Quest 2 controller and a keyboard.                                                                                                                                                      | Pickle format                                             |
| RH20T           | Includes pairs of human and robot demonstrations for each task. Also contains single sentence task descriptions.                                                                                                                               | Compressed tarball of MP4 and NumPy data                  |
| Dobb-E          | An open-source framework using a low-cost tool ("The Stick") to collect the "Homes of New York (HoNY)" dataset for teaching robots household tasks via imitation learning.                                                                     | Custom (includes RGB/depth videos and action annotations) |
| MT-Opt          | Multi-robot collective learning system for data collection. Pick Place tasks with 7 Kukas.                                                                                                                                                     | Offline RL dataset format                                 |
| RoboNet         | Open database for sharing robot experience from 7 different robot platforms and institutions.                                                                                                                                                  | TFRecord                                                  |
| BC-Z            | A large-scale imitation learning system for zero-shot and few-shot generalization to novel manipulation tasks not seen during training. [[7]](https://hyper.ai/en/datasets/35172)[[8]](https://proceedings.mlr.press/v164/jang22a/jang22a.pdf) | RLDS (via TFDS)                                           |


<details>
<summary><i>Full paper citations </i></summary>

- **DROID**, DROID: A Large-Scale Robot Learning Dataset with High-Quality Demonstrations, 2024.03. [ğŸ“„ Paper](https://arxiv.org/abs/2403.12945) [ğŸ’» Website](https://droid-dataset.github.io/) [ğŸ’¾ Dataset](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing)
    
- **Mobile ALOHA**, Mobile ALOHA: A Low-Cost, Full-Body Mobile Robot for Daily Living, 2024.01. [ğŸ“„ Paper](https://arxiv.org/abs/2401.02117) [ğŸ’» Website](https://mobile-aloha.github.io/) [ğŸ’¾ Dataset](https://drive.google.com/drive/folders/1FP5eakcxQrsHyiWBRDsMRvUfSxeykiDc)
    
- **BridgeData V2**, BridgeData V2: A Large-Scale Dataset for Learning General-Purpose Robot Manipulation Policies, 2023.08. [ğŸ“„ Paper](https://arxiv.org/abs/2308.12952) [ğŸ’» Website](https://rail-berkeley.github.io/bridgedata/) [ğŸ’¾ Dataset](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/)
    
- **RoboSet**, RoboSet: A Large-Scale Dataset for Robot Manipulation with Diverse Objects and Actions, 2023.09. [ğŸ“„ Paper](https://arxiv.org/abs/2309.01918) [ğŸ’» Website](https://robopen.github.io/roboset/) [ğŸ’¾ Dataset](https://github.com/vikashplus/robohive/wiki/7.-Datasets)
    
- **Furniture Bench**, Furniture Bench: A Benchmark for Assembling Furniture with a Robot, 2023.05. [ğŸ“„ Paper](https://arxiv.org/abs/2305.12821) [ğŸ’» Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) [ğŸ’¾ Dataset](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)
    
- **RH20T**, RH20T: A Benchmark for Robot Hand Dexterous Manipulation, 2023.07. [ğŸ“„ Paper](https://arxiv.org/abs/2307.00595) [ğŸ’» Website](https://rh20t.github.io/) [ğŸ’¾ Dataset](https://rh20t.github.io/#download)
    
- **Dobb-E**, Dobb-E: A Dataset for Robot Manipulation with Unseen Objects, 2023.11. [ğŸ“„ Paper](https://arxiv.org/abs/2311.16098) [ğŸ’» Website](https://dobb-e.com/) [ğŸ’¾ Dataset](https://github.com/notmahi/dobb-e/)
    
- **MT-Opt**, MT-Opt: A Multi-Task, Multi-Modal Robot Learning Framework, 2021.04. [ğŸ“„ Paper](https://arxiv.org/abs/2104.08212) [ğŸ’» Website](https://karolhausman.github.io/mt-opt/) [ğŸ’¾ Dataset](https://www.tensorflow.org/datasets/catalog/mt_opt)
    
- **RoboNet**, RoboNet: A Dataset for Robot Grasping and Manipulation, 2019.10. [ğŸ“„ Paper](https://arxiv.org/abs/1910.11215) [ğŸ’» Website](https://www.robonet.wiki/) [ğŸ’¾ Dataset](https://github.com/SudeepDasari/RoboNet/wiki/Getting-Started)
    
- **BC-Z**, BC-Z: A Large-Scale Robot Learning Benchmark for Dexterous Manipulation, 2022.02. [ğŸ“„ Paper](https://arxiv.org/abs/2202.02005) [ğŸ’» Website](https://sites.google.com/view/bc-z/home) [ğŸ’¾ Dataset](https://www.kaggle.com/datasets/google/bc-z-robot)

</details>

---

## Combined (Sim + Real)

| NAME      | Links                                                                                                                                                                                                                                 | YEAR | SIZE                                                                                                                         | MODALITIES                                                                                                                                   | ROBOTS | UNIVERSITY / ORG                                       | DATA FORMAT | NOTES                                                                                              |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ---------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------------------------------ | ----------- | -------------------------------------------------------------------------------------------------- |
| RoboTurk  | ğŸŒ [Website](https://roboturk.stanford.edu/index.html) ğŸ“„ [Paper](https://arxiv.org/abs/1811.02790)  ğŸ’¾ [Data Real](https://roboturk.stanford.edu/dataset_real.html)<br>ğŸ’¾ [Data Sim](https://roboturk.stanford.edu/dataset_sim.html) | 2019 | ***Real*** <br>ğŸ•¹ï¸ 2144 demonstrations<br>***Sim*** <br>ğŸ•¹ï¸ 1070 SawyerPickPlace<br>ğŸ•¹ï¸1147 SawyerNutAssembly demonstrations | ***Real***  <br>ğŸ–¼ï¸ Multiple camera views and depth data<br>ğŸ¤– control data from the user and joint data from the robot<br> ***Sim***<br> ğŸ¤– | Sawyer | Stanford University                                    | HDF5, MP4   | Real datasets contains laundry layout, tower creation, and object search tasks done (54 users).    |
| RoboMimic | ğŸŒ [Website](https://robomimic.github.io/) ğŸ“„ [Paper](https://arxiv.org/abs/2108.03298)  ğŸ’¾ [Data](https://robomimic.github.io/docs/datasets/robomimic_v0.1.html)                                                                     | 2021 | ğŸ§© 5 tasks<br>ğŸ•¹ï¸ 500 human generated  trajectories<br>ğŸ•¹ï¸5400 machine generated trajectories<br>                            | ğŸ¤– Robo                                                                                                                                      | Franka | Stanford University, The University of Texas at Austin | HDF5        | Dataset contains very simple tasks Lift, PickandPlace. Data collected using the RoboTurk platfrom. |
<details>
<summary><i>Full paper citations </i></summary>

- **RoboTurk**, RoboTurk: A Crowd-Sourcing Platform for High-Quality Robot Data, 2018.11, CoRL 2018. [ğŸ“„ Paper](https://arxiv.org/abs/1811.02790) [ğŸ’» Website](https://roboturk.stanford.edu/index.html) [ğŸ’¾ Dataset](https://roboturk.stanford.edu/dataset_real.html)
    
- **RoboMimic**, RoboMimic: A Dataset and Benchmark for Vision-Based Robot Manipulation, 2021.08. [ğŸ“„ Paper](https://arxiv.org/abs/2108.03298) [ğŸ’» Website](https://robomimic.github.io/) [ğŸ’¾ Dataset](https://robomimic.github.io/docs/datasets/robomimic_v0.1.html)
</details>

---

## Dataset Collections

| NAME              | Links                                                                                                                                                                                                                                                                                                                                                                                                             | YEAR | SIZE                                 | MODALITIES                                                                | ROBOTS                                                | UNIVERSITY / ORG                                                                                                                                                                  | DATA FORMAT | NOTES                                                                                                                                                                                                                                                                                                           |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------------------------------ | ------------------------------------------------------------------------- | ----------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Open-X Embodiment | ğŸŒ [Website](https://robotics-transformer-x.github.io/)  ğŸ“„ [Paper](https://arxiv.org/abs/2310.08864)  ğŸ’¾[Data](https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb#scrollTo=N2Efw2aHVfSX)ğŸ“Š[Dataset overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0) | 2023 | 71 datasets (last checked 15.9.2025) | ğŸ–¼ï¸ RGB, depth cameras  <br>ğŸ—£ï¸ Language annotations<br>ğŸ¤– Proprioception | Franka, Spot, Hello Stretch, UR5, Sawyer, ViperX, ... | Arizona State University,<br>Carnegie Mellon University ,Columbia University,<br>ETH ZÃ¼rich, Google DeepMind, and many [other](https://robotics-transformer-x.github.io/)<br><br> | RLDS        | Information about exact modalities and robots in [google sheet overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0). To get SOTA results not all datasets are used (filtering is required). Open-X provides unified download and format of datasets. |
<details>
<summary><i>Full paper citations </i></summary>

- **Open-X Embodiment**, Open-X Embodiment: A New Benchmark for Large-Scale Robot Learning, 2023.10. [ğŸ“„ Paper](https://arxiv.org/abs/2310.08864) [ğŸ’» Website](https://robotics-transformer-x.github.io/) [ğŸ’¾ Dataset](https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb#scrollTo=N2Efw2aHVfSX)
</details>

---
## Benchmarks

| NAME               | YEAR | SIM/REAL | TASK TYPE                                | SIMULATOR / PLATFORM | METRICS                                                                                                  | ROBOTS                           | UNIVERSITY / ORG            | OTHER NOTES                                                    |
| ------------------ | ---- | -------- | ---------------------------------------- | -------------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------- | --------------------------- | -------------------------------------------------------------- |
| **LIBERO**         | 2023 | Sim      | ğŸ–¼ï¸ Vision, ğŸ—£ï¸ Language, ğŸ¤– Action      | RoboSuite            | FWT (forward transfer), NBT (negative backward transfer), and<br>AUC (area under the success rate curve) | Franka                           | UT Austin, Tsinghua Univers | Lifelong learning in decision making and rocedurally generated |
| **ALOHA**          | 2023 | Real     | ğŸ–¼ï¸ Vision, ğŸ› ï¸ Force/Tactile, ğŸ¤– Action | Real robots (Franka) | Task success, Contact accuracy                                                                           | Low-cost dual-arm robot (Franka) | Stanford                    | Bimanual manipulation benchmark                                |
| **FurnitureBench** | 2023 | Real     | ğŸ–¼ï¸ Vision, ğŸ¤– Action                    | Real Franka robot    | Assembly success, Completion time                                                                        | Franka Panda                     | KAIST                       | Long-horizon assembly tasks                                    |
| **BEHAVIOR-1K**    | 2022 | Sim+Real | ğŸ–¼ï¸ Vision, ğŸ¤– Action                    | iGibson 2.0 + Robots | Task success, Diversity coverage                                                                         | Franka, Fetch, TurtleBot         | Stanford                    | 1,000 household activities benchmark                           |
| **CALVIN**         | 2022 | Real     | ğŸ–¼ï¸ Vision, ğŸ—£ï¸ Language, ğŸ¤– Action      | Real Franka robots   | Success rate, Language grounding                                                                         | Franka Panda                     | TU Dresden                  | Language-conditioned manipulation                              |
| **Habitat 2.0**    | 2021 | Sim+Real | ğŸ–¼ï¸ Vision, ğŸ—£ï¸ Language, ğŸ¤– Action      | Habitat Sim          | Navigation success, Rearrangement metrics                                                                | LoCoBot, Fetch                   | FAIR, Georgia Tech          | Rearrangement + embodied AI tasks                              |
| **ALFRED**         | 2020 | Sim      | ğŸ–¼ï¸ Vision, ğŸ—£ï¸ Language, ğŸ¤– Action      | AI2-THOR             | Goal-condition success (GC-SR), Path length                                                              | Virtual agents (AI2-THOR)        | Allen Institute for AI      | Instruction-following tasks                                    |
| **RLBench**        | 2020 | Sim      | ğŸ–¼ï¸ Vision, ğŸ—£ï¸ Language, ğŸ¤– Action      | PyRep (MuJoCo-like)  | Task success, Generalization                                                                             | Franka Panda (sim)               | Imperial College London     | Multi-task manipulation benchmark                              |
| **BEHAVIOR-100**   | 2020 | Sim+Real | ğŸ–¼ï¸ Vision, ğŸ¤– Action                    | iGibson              | Task success, Diversity coverage                                                                         | Fetch, LoCoBot                   | Stanford                    | Subset of BEHAVIOR-1K                                          |
| **Meta-World**     | 2019 | Sim      | ğŸ“ State, ğŸ¤– Action                      | MuJoCo               | Success rate, Generalization                                                                             | Sawyer (sim)                     | UC Berkeley                 | Multi-task & meta-RL tasks                                     |
| **VirtualHome**    | 2018 | Sim      | ğŸ—£ï¸ Language, ğŸ–¼ï¸ Vision, ğŸ¤– Action      | Unity3D              | Script success, Task diversity                                                                           | Virtual agents (avatars)         | MIT, Stanford               | Household activity simulation benchmark                        |
<details>
<summary><i>Full paper citations </i></summary>

- **LIBERO**, LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning, 2023.06. [ğŸ“„ Paper](https://arxiv.org/abs/2306.03310) [ğŸ’» Website](https://libero-project.github.io/datasets) [ğŸ› ï¸ Code](https://libero-project.github.io/datasets)
    
- **ALOHA**, Mobile ALOHA: A Low-Cost, Full-Body Mobile Robot for Daily Living, 2024.01. [ğŸ“„ Paper](https://arxiv.org/abs/2401.02117) [ğŸ’» Website](https://mobile-aloha.github.io/) [ğŸ› ï¸ Code](https://drive.google.com/drive/folders/1FP5eakcxQrsHyiWBRDsMRvUfSxeykiDc)
    
- **FurnitureBench**, Furniture Bench: A Benchmark for Assembling Furniture with a Robot, 2023.05. [ğŸ“„ Paper](https://arxiv.org/abs/2305.12821) [ğŸ’» Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) [ğŸ› ï¸ Code](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)
    
- **BEHAVIOR-1K**, BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation, 2024.03. [ğŸ“„ Paper](https://arxiv.org/abs/2403.09227) [ğŸ’» Website](https://behavior.stanford.edu/challenge/index.html) [ğŸ› ï¸ Code](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)
    
- **CALVIN**, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, 2021.12, IEEE Robotics and Automation Letters. [ğŸ“„ Paper](https://arxiv.org/abs/2112.03227) [ğŸ’» Website](http://calvin.cs.uni-freiburg.de/) [ğŸ› ï¸ Code](https://github.com/mees/calvin)
    
- **Habitat 2.0**, Habitat 2.0: Training Home Assistants to Rearrange their Habitat, 2021.06, NeurIPS 2021. [ğŸ“„ Paper](https://arxiv.org/abs/2106.14405) [ğŸ’» Website](https://aihabitat.org/) [ğŸ› ï¸ Code](https://github.com/facebookresearch/habitat-lab)
    
- **ALFRED**, ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks, 2019.12, CVPR 2020. [ğŸ“„ Paper](https://arxiv.org/abs/1912.01734) [ğŸ’» Website](https://askforalfred.com/) [ğŸ› ï¸ Code](https://github.com/askforalfred/alfred)
    
- **RLBench**, The Robot Learning Benchmark & Learning Environment, 2019.09, CoRL 2019. [ğŸ“„ Paper](https://arxiv.org/abs/1909.12271) [ğŸ’» Website](https://sites.google.com/view/rlbench) [ğŸ› ï¸ Code](https://github.com/stepjam/RLBench)
    
- **BEHAVIOR-100**, BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments, 2021.08, CoRL 2021. [ğŸ“„ Paper](https://arxiv.org/abs/2108.03332) [ğŸ’» Website](https://behavior.stanford.edu/behavior_100/overview.html) [ğŸ› ï¸ Code](https://behavior.stanford.edu/behavior_100/dataset.html)
    
- **Meta-World**, Meta-World: A Benchmark for Evaluating Multi-Task and Meta-Learning in Robotics, 2019.10, ICRA 2020. [ğŸ“„ Paper](https://arxiv.org/abs/1910.10897) [ğŸ’» Website](https://meta-world.github.io/) [ğŸ› ï¸ Code](https://github.com/rlworkgroup/metaworld)
    
- **VirtualHome**, VirtualHome: Simulating Household Activities via Programs, 2018.06, CVPR 2018. [ğŸ“„ Paper](http://virtual-home.org/paper/virtualhome.pdf) [ğŸ’» Website](http://virtual-home.org/) [ğŸ› ï¸ Code](https://github.com/xavierpuigf/virtualhome)
</details>

---
# Physics-Based Simulation Frameworks

| Framework                                                                                            | Associated Benchmarks & Environments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **[NVIDIA Isaac Sim](https://developer.nvidia.com/isaac/sim)**                                       | [BEHAVIOUR-1K](https://behavior.stanford.edu/); [ARNOLD](https://arnold-benchmark.github.io/); [Isaac Lab ](https://isaac-sim.github.io/IsaacLab/main/index.html)[(ORBIT)](https://isaac-orbit.github.io/); [Isaac Lab Mimic](https://isaac-sim.github.io/IsaacLab/main/source/overview/teleop_imitation.html)                                                                                                                                                                                                                                                                                                                                                                                                              |
| [**NVIDIA Isaac Gym**](https://developer.nvidia.com/isaac-gym)                                       | [Furniture Bench](https://clvrai.github.io/furniture-bench/); [AutoMate](https://bingjietang718.github.io/automate/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **[MuJoCo](https://github.com/google-deepmind/mujoco)**                                              | [Robosuite](https://robosuite.ai/) ([Robomimic](https://robomimic.github.io/), [LIBERO](https://libero-project.github.io/main.html), [MuBle](https://michaal94.github.io/MuBlE/), [CompoSuite](https://github.com/Lifelong-ML/CompoSuite), [RoboCasa](https://robocasa.ai/)); [Meta-World ](https://meta-world.github.io/)([Continual World](https://sites.google.com/view/continualworld)); [MuJoBan, MuJoXO, MuJoGo](https://github.com/google-deepmind/deepmind-research/tree/master/physics_planning_games);  [ALOHA sim](https://github.com/google-deepmind/mujoco_menagerie/tree/main/aloha/), [VLABench](https://vlabench.github.io/); [Gymnasium-Robotics](https://github.com/Farama-Foundation/Gymnasium-Robotics) |
| **[SAPIEN](https://sapien.ucsd.edu/)**                                                               | [ManiSkill 3](https://www.maniskill.ai/); [ManiSkill 2](https://maniskill2.github.io/) ( [SimplerEnv](https://simpler-env.github.io/), [ClevrSkills](https://github.com/Qualcomm-AI-research/ClevrSkills), [VLATest](https://github.com/ma-labo/VLATest))                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **[Unity](https://unity.com/)**                                                                      | [AI2-THOR](https://ai2thor.allenai.org/) ([ManipulaTHOR](https://prior.allenai.org/projects/manipulathor), [ProcTHOR](https://procthor.allenai.org/), [ALFRED](https://askforalfred.com/))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| **[iGibson 2.0](https://svl.stanford.edu/igibson/)**                                                 | [BEHAVIOUR-100](https://behavior.stanford.edu/behavior_100/overview.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **[PyBullet](https://pybullet.org/wordpress/)**                                                      | [CausalWorld](https://sites.google.com/view/causal-world/home), [Habitat-Sim](https://github.com/facebookresearch/habitat-sim), [Calvin](http://calvin.cs.uni-freiburg.de/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| [**CoppeliaSim** ](https://www.coppeliarobotics.com/)**([PyRep](https://github.com/stepjam/PyRep))** | [RLBench](https://sites.google.com/view/rlbench) ([AGNOSTOS](https://jiaming-zhou.github.io/AGNOSTOS/))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [**Webots**](https://cyberbotics.com/)                                                               | [deepbots](https://github.com/aidudezzz/deepbots)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| [**Drake**](https://drake.mit.edu/)                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| [**Genesis**](https://genesis-embodied-ai.github.io/)                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Text-Oriented**                                                                                    | [TextWorld](https://github.com/Microsoft/TextWorld); [ALFWorld](https://alfworld.github.io/); [Tales](https://microsoft.github.io/tale-suite/); [WebShop](https://webshop-pnlp.github.io/); [PCA-Bench](https://arxiv.org/pdf/2310.02071); [PlanBench](https://github.com/karthikv792/LLMs-Planning)                                                                                                                                                                                                                                                                                                                                                                                                                        |

---
# Models

- **RoboPen**, RoboAgent: Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking, 2023.09. [ğŸ“„ Paper](https://arxiv.org/abs/2309.01918) [ğŸ’» Website](https://robopen.github.io/) [ğŸ› ï¸ Code](https://robopen.github.io/)
    
- **Octo**, Octo: An Open-Source Generalist Robot Policy, 2024.05. [ğŸ“„ Paper](https://www.semanticscholar.org/paper/1d2753d74025e7a71594506623be81f18b073adb) [ğŸ› ï¸ Code](https://www.google.com/search?q=https://github.com/octo-robotics/octo)
    
- **Hi Robot**, Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models, 2025.02. [ğŸ“„ Paper](https://arxiv.org/abs/2502.19417) [ğŸ’» Website](https://www.themoonlight.io/en/review/hi-robot-open-ended-instruction-following-with-hierarchical-vision-language-action-models)
    
- **Do As I Can, Not As I Say (SayCan)**, Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, 2022.04. [ğŸ“„ Paper](https://research.google/pubs/do-as-i-can-not-as-i-say-grounding-language-in-robotic-affordances/) [ğŸ’» Website](https://say-can.github.io/)
    
- **Gemini Robotics**, Gemini Robotics: Bringing AI into the Physical World, 2025.03. [ğŸ“„ Paper](https://arxiv.org/abs/2503.20020) [ğŸ’» Website](https://www.google.com/search?q=https://research.google/pubs/gemini-robotics-bringing-ai-into-the-physical-world/)
    
- **RDT-1B**, RDT-1B: A Diffusion Foundation Model for Bimanual Manipulation, 2024.10. [ğŸ“„ Paper](https://www.alphaxiv.org/overview/2410.07864v1) [ğŸ’» Website](https://www.google.com/search?q=https://rdt-1b.github.io/)
    
- **RT-X**, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, 2023.10. [ğŸ“„ Paper](https://arxiv.org/abs/2310.08864) [ğŸ’» Website](https://robotics-transformer-x.github.io/) [ğŸ› ï¸ Code](https://github.com/kyegomez/RT-X)
    
- **pi0**, Ï€0: A Foundation Model for Robotics with Sergey Levine, 2025.02. [ğŸ“„ Paper](https://www.youtube.com/watch?v=5mY71rGXAkM) [ğŸ’» Website](https://physical-intelligence.com/) [ğŸ› ï¸ Code](https://github.com/Physical-Intelligence/openpi)
    
- **OpenVLA**, OpenVLA: An Open-Source Vision-Language-Action Model, 2025.05. [ğŸ“„ Paper](https://proceedings.mlr.press/v270/kim25c.html) [ğŸ’» Website](https://openvla.github.io/) [ğŸ› ï¸ Code](https://www.google.com/search?q=https://huggingface.co/collections/open-vla)
    
- **MolmoAct**, MolmoAct: Action Reasoning Models that can Reason in Space, 2025.08. [ğŸ“„ Paper](https://arxiv.org/abs/2508.07917) [ğŸ’» Website](https://www.google.com/search?q=https://allenai.org/molmo/) [ğŸ› ï¸ Code](https://github.com/allenai/molmoact)


---
# Papers to include
- datasets:
    - Papers without code:
**[https://momagen-rss.github.io/](https://momagen-rss.github.io/)** (16.9.2025 no code yet)
    - https://imaei.github.io/project_pages/ario/

- benchmarks:
    - https://robo-arena.github.io/

- models:
    - https://arxiv.org/pdf/2509.01819

# List of used platforms
- Galaxea R1
- GR-1
- Sawyer
- Unitree G1
- Unitree H1
- Franka Panda
- Franka Emika Panda
- UR10e
- Allegro Hand
- Allegro
- XArm
- Fetch
- LoCoBot
- TurtleBot
- Baxter
- WidowX 250
- Hello Robot Stretch
- Kuka
- AI2-THOR Robots (virtual agents)







