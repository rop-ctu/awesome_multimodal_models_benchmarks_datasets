# Awesome Robotics Datasets, Benchmarks & Models

Curated list of simulation and real-world datasets for robot learning, plus benchmarks, simulators, and notable models.

## Table of Contents
- [Other Awesome Lists](#other-awesome-lists)
- [Datasets](#datasets)
  - [Simulated](#simulated)
  - [Real-World](#real-world)
  - [Combined (Sim + Real)](#combined-sim--real)
  - [Dataset Collections](#dataset-collections)
- [Benchmarks](#benchmarks)
- [Physics-Based Simulation Frameworks](#physics-based-simulation-frameworks)
- [Models](#models)
- [Papers to include](#papers-to-include)

## Other Awesome Lists
- [Awesome-VLA-Robotics](https://github.com/ksDreamer/Awesome-VLA-Robotics)
- [Awesome-VLA-Papers](https://github.com/Psi-Robot/Awesome-VLA-Papers)
- [Awesome-VLA](https://github.com/Orlando-CS/Awesome-VLA)
- [awesome-ai-robotics](https://github.com/tc-huang/awesome-ai-robotics)
- [Awesome-Robot-Learning](https://github.com/RayYoh/Awesome-Robot-Learning)
- [Awesome-Implicit-NeRF-Robotics](https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Robotics)
- [awesome-robotics-datasets](https://mint-lab.github.io/awesome-robotics-datasets/)
- [awesome-isaac-gym](https://github.com/robotlearning123/awesome-isaac-gym)
- [awesome-robotics](https://github.com/ahundt/awesome-robotics)
- [awesome-robotics papers](https://github.com/ahundt/awesome-robotics/blob/master/papers.md)
- [awesome-robotics-libraries](http://jslee02.github.io/awesome-robotics-libraries/)
- [awesome-human-robot-interaction](https://github.com/Po-Jen/awesome-human-robot-interaction)
- [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning)
- [Awesome-object-pose-estimation](https://github.com/YoungXIAO13/ObjectPoseEstimationSummary)

---
# Datasets

## Simulated
| **NAME**                                                        | YEAR | SIZE                                                                                                                            | MODALITIES                                                                                                                                                                          | ROBOTS                                                                                                                                         | UNI / ORG                                                                    | Links                                                                                                                                                                                                                                                                                                                                                  |
| --------------------------------------------------------------- | ---- | ------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **BEHAVIOR-1k <br>(2025 BEHAVIOR Challenge)**                   | 2025 | ğŸ§© 50 tasks<br>full-length household<br><br>ğŸ•¹ï¸ 10,000<br>teleop. demos.                                                        | ğŸ–¼ï¸  RGB, depth, and instance segmentation images<br><br>ğŸ¤– Robot state, action, task information                                                                                   | Galaxea R1 robot                                                                                                                               | Stanford                                                                     |  [ğŸŒ&nbsp;Website](https://behavior.stanford.edu/challenge/index.html)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2403.09227)<br>[ğŸ’¾&nbsp;Data](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)                                                                                                                                                           |
| **DexMimicGen**                                                 | 2025 | ğŸ•¹ï¸ 21,000+ <br>demos.<br><br>ğŸ§© 9 tasks                                                                                        | ğŸ–¼ï¸ Image <br><br>ğŸ¤– Robot state, action                                                                                                                                            | GR-1 <br>humanoid, bimanual <br>Panda<br>arms with dexterous hands                                                                             | NVIDIA Research,<br>UT Austin, <br>UC San Diego                              |  [ğŸŒ&nbsp;Website](https://dexmimicgen.github.io/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2410.24185)<br>[ğŸ’¾&nbsp;Data](https://huggingface.co/datasets/MimicGen/dexmimicgen_datasets/tree/main)                                                                                                                                                                        |
| **Lerobot (Meta-World+)**                                       | 2025 | ğŸ§© 50 manip.<br>tasks                                                                                                           | ğŸ–¼ï¸ RGB-D image<br><br>ğŸ¤– Robot action,  Environment state                                                                                                                          | Sawyer                                                                                                                                         | Farama foundation, LeRobot                                                    |   [ğŸŒ&nbsp;Website](https://meta-world.github.io/) <br>[ğŸ’¾&nbsp;Data](https://huggingface.co/datasets/lerobot/metaworld_mt50)                                                                                                                                                                                                                                          |
| **ManiSkill 3 (Datasets creation inprogress as of 1.9.2025)**   | 2024 | ğŸ  16 envs.                                                                                                                     | ğŸ¤– demos. (motion planning, teleop) <br> <br> ğŸ–¼ï¸ RGB-D, point clouds                                                                                                               | Unitree <br>G1/H1,<br>Panda, <br>UR10e,<br>Allegro<br>Hand, <br>XArm, <br>[etc.](https://maniskill.readthedocs.io/en/latest/robots/index.html) | UC San Diego <br>(SU Lab)                                                    |   [ğŸŒ&nbsp;Website](https://www.maniskill.ai/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2410.00425)<br>[ğŸ’¾&nbsp;Data](https://huggingface.co/datasets/haosulab/ManiSkill_Demonstrations)                                                                                                                                                                                     |
| **LIBERO**                                                      | 2023 | ğŸ§© 130 tasks                                                                                                                    | ğŸ–¼ï¸ RGB images from workspace and wrist cameras <br><br>ğŸ¤– Proprioception <br><br>ğŸ—£ï¸ Language task specifications <br><br>ğŸ“ PDDL scene descriptions                               | Panda                                                                                                                                          | UT Austin, Tsinghua U                                                        |  [ğŸŒ&nbsp;Website](https://libero-project.github.io/datasets)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2306.03310)<br>[ğŸ’¾&nbsp;Data](https://libero-project.github.io/datasets)<br>                                                                                                                                                                                      |
| **Habitat**                                                     | 2023 | Embodied Question Answering,<br>Visual <br>Language Navigation, Rearrange <br>Pick,<br>Object <br>and <br>point goal navigation | ğŸ—£ï¸ Scene description and language annotation (EQA, VLN)                                                                                                                            | Fetch, <br>Virtual <br>Human<br>Agents                                                                                                         | Facebook AI, Georgia Tech, <br>UC Berkeley, Simon Fraser, <br>Intel Research | (2.0)<br> [ğŸŒ&nbsp;Website](https://aihabitat.org/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2106.14405)<br>(3.0)<br>  [ğŸŒ&nbsp;Website](https://aihabitat.org/habitat3/) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.13724)<br>ğŸ’¾ [Data](https://huggingface.co/ai-habitat) |
| **MimicGen**                                                    | 2023 | ğŸ•¹ï¸ 48,000 <br>task demos.<br><br>ğŸ§©12 tasks                                                                                    | ğŸ–¼ï¸ Image<br><br>ğŸ¤– Robot state, action, task observation                                                                                                                           | Panda,<br>Sawyer,<br>IIWA,<br>UR5e                                                                                                             | The UTexas at Austin                                                         |   [ğŸŒ&nbsp;Website](https://mimicgen.github.io/) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.17596) [ğŸ’¾&nbsp;Data](https://huggingface.co/datasets/amandlek/mimicgen_datasets)                                                                                                                                                                                           |
| **ManiSkill 2**                                                 | 2023 | ğŸ§© 20 tasks                                                                                                                     | ğŸ¤– demos. (scene and robot states)<br><br>ğŸ–¼ï¸ Replay tools (point cloud, RGB-D)                                                                                                     | Panda                                                                                                                                          | UC San Diego, Tsinghua Uni.                                                  |   [ğŸŒ&nbsp;Website](https://maniskill2.github.io/)[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2302.04659)ğŸ’¾ [Data](https://huggingface.co/datasets/haosulab/ManiSkill2)                                                                                                                                                                                                  |
| **CALVIN (Composing Actions from Language and Vision) Dataset** | 2022 | ğŸ  4 envs.<br>(24 hours of teleoped. â€œplayâ€)                                                                                    | ğŸ–¼ï¸ RGB-D images from both static and gripper-mounted cameras<br><br>ğŸ—£ï¸ Language Annotations<br><br>ğŸ¤– Action (tool center point), State Observation<br><br>ğŸ¤ Tactile image input | Panda                                                                                                                                          | UFreiburg                                                                    |   [ğŸŒ&nbsp;Website](http://calvin.cs.uni-freiburg.de/)  [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2112.03227)  ğŸ’¾ [Data](https://github.com/mees/calvin/tree/main/dataset)                                                                                                                                                                                             |
| **RLBench (Perceiver-Actor Dataset)**                           | 2022 | Dataset <br>generator                                                                                                           | ğŸ–¼ï¸ Multiple RBG, depth and mask camera views <br><br>ğŸ¤– Episode variation description                                                                                              | Panda                                                                                                                                          | UWashington, NVIDIA                                                          | ***Perceiver-Actor*** <br>  [ğŸŒ&nbsp;Website](https://peract.github.io/)[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2209.05451)<br>***RLBench***<br>  [ğŸŒ&nbsp;Website](https://sites.google.com/view/rlbench)[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1909.12271)                                                                                                                      |
| **Mobile Manipulation RoboTurk (MoMaRT)**                       | 2021 | ğŸ•¹ï¸1200 demos. <br><br>ğŸ§© 5 tasks                                                                                               | ğŸ–¼ï¸ Normalized LIDAR scan, RGB, depth<br><br>ğŸ¤– Objects states, robot actions, rewards and proprioception                                                                           | Fetch                                                                                                                                          | Stanford                                                                     |   [ğŸŒ&nbsp;Website](https://sites.google.com/view/il-for-mm/home) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2112.05251) [ğŸ’¾&nbsp;Data](https://sites.google.com/view/il-for-mm/datasets#h.ko0ilbky4y5u)                                                                                                                                                                     |
| **BEHAVIOR-100 Dataset**                                        | 2021 | ğŸ•¹ï¸ 500 <br>VR demos.                                                                                                           | ğŸ–¼ï¸ Semantic segmentation, instance segmentation, RGB, depth, highlight<br><br>ğŸ¤–Proprioception, agent action                                                                       | VR agent                                                                                                                                       | Stanford                                                                     |   [ğŸŒ&nbsp;Website](https://behavior.stanford.edu/behavior_100/overview.html)  [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2108.03332)  <br>ğŸ’¾ [Data](https://behavior.stanford.edu/behavior_100/dataset.html)                                                                                                                                                           |
| **D4RL**                                                        | 2020 | ğŸ§©4 <br>Franka Kitchen with<br>4 subtasks                                                                                       | ğŸ¤– robot and scene state observation                                                                                                                                                | Ant, <br>Hopper, Walker2D, <br>Panda                                                                                                           | Google,<br>UC Berkeley                                                       |   [ğŸŒ&nbsp;Website](https://sites.google.com/view/d4rl-anonymous/) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2004.07219) ğŸ’¾ [Data](https://minari.farama.org/datasets/D4RL/kitchen/)                                                                                                                                                                                   |
| **ALFRED**                                                      | 2020 | ğŸ•¹ï¸ 8,055 <br>expert demos. <br>(25,743<br>language directives)                                                                 | ğŸ–¼ï¸ Resnet18 features, video sequence<br><br>ğŸ—£ï¸ Language <br><br>ğŸ¤– Action                                                                                                         | AI2-THOR <br>agents                                                                                                                            | UWashington, Allen Institute for AI                                          |   [ğŸŒ&nbsp;Website](https://askforalfred.com/)  [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1912.01734) [ğŸ’¾&nbsp;Data](https://github.com/askforalfred/alfred/tree/master/data)                                                                                                                                                                                                |
| **VirtualHome**                                                 | 2018 | ğŸ§©2821<br>Activity <br>Programs,  <br><br>ğŸ§© 5193 Synthetic Programs                                                            | ğŸ—£ï¸ Semantic Segmentation, Natural Language Descriptions <br><br>ğŸ–¼ï¸ RGB-D data<br><br>ğŸ¤– Pose information                                                                          | Virtual <br>avatars                                                                                                                            | MIT, U Toronto                                                               |   [ğŸŒ&nbsp;Website](http://virtual-home.org/)[ğŸ“„&nbsp;Paper](http://virtual-home.org/paper/virtualhome.pdf) [ğŸ’¾&nbsp;Data](https://github.com/xavierpuigf/virtualhome/blob/master/virtualhome/dataset/README.md)                                                                                                                                                         |
<details>
  <summary><i>Additional notes</i></summary>

<table>
  <thead>
    <tr>
      <th><strong>NAME</strong></th>
      <th>DATA FORMAT</th>
      <th>NOTES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>BEHAVIOR-1k</strong> <br>(2025 BEHAVIOR Challenge)</td>
      <td>LeRobot format (parquet)</td>
      <td>Datasets of human-grounded dexterous bimanual manipulations in house-scale scenes part of the <a href="https://foundation-models-meet-embodied-agents.github.io/eai_challenge/">2025 EAI Challenge</a>.</td>
    </tr>
    <tr>
      <td><strong>DexMimicGen</strong></td>
      <td>HDF5</td>
      <td>Large-scale automated data<br>generation system from small number of human demos. for bi-manual manipulation.</td>
    </tr>
    <tr>
      <td><strong>Lerobot (Meta-World+)</strong></td>
      <td>LeRobot format (parquet)</td>
      <td>Single arm manipulation tasks. Generated using expert policies in the Meta-world simulation benchmarks.</td>
    </tr>
    <tr>
      <td><strong>ManiSkill 3 (Datasets creation inprogress as of 1.9.2025)</strong></td>
      <td>HDF5, JSON</td>
      <td>Single arm manipulation tasks (e.g peg insertion) and some simple humanoid (Unitree G1) and mobile manipulation tasks. In development; most environments are missing demos.</td>
    </tr>
    <tr>
      <td><strong>LIBERO</strong></td>
      <td>HDF5</td>
      <td>Single arm pick and place tasks, human-teleoperated demos. inside simulation.</td>
    </tr>
    <tr>
      <td><strong>Habitat</strong></td>
      <td>JSON</td>
      <td>Habitat 2.0 contains Scans and CAD Replicas (with simulation framework) of interiors. Embodied datasets mainly came with Habitat 3.0 datasets. Relevant Docs for Sim in Habitat 2.0</td>
    </tr>
    <tr>
      <td><strong>MimicGen</strong></td>
      <td>HDF5</td>
      <td>A system for automatically synthesizing large-scale datasets from a small number of human demos. by adapting them to new scene configurations, object instances, and robot arms.</td>
    </tr>
    <tr>
      <td><strong>ManiSkill 2</strong></td>
      <td>HDF5, JSON</td>
      <td>Replaced by ManiSkill 3 (which is still in beta).</td>
    </tr>
    <tr>
      <td><strong>CALVIN</strong> (<em>C</em>omposing <em>A</em>ctions from <em>L</em>anguage and <em>Vi</em>sio<span style="font-variant: small-caps;">n</span>) Dataset</td>
      <td>NumPy files</td>
      <td>Single arm manipulation with multiple drawers. Long horizon in terms of sequence length. Data colected with HTC Vive VR headset.</td>
    </tr>
    <tr>
      <td><strong>RLBench (Perceiver-Actor Dataset)</strong></td>
      <td>PNG, Pickle format</td>
      <td>Generated simulation single arm datasets using the RLBench datasets generation utility (using template policies). Data generator example: pregenerated 18 tasks in Perceiver-actor ğŸ’¾ <a href="https://huggingface.co/datasets/hqfang/rlbench-18-tasks">Data</a></td>
    </tr>
    <tr>
      <td><strong>Mobile Manipulation RoboTurk (MoMaRT)</strong></td>
      <td>HDF5</td>
      <td>Simulated mobile manipulation in kitchen environments (e.g setting table from dishwasher, table cleanup to sink).</td>
    </tr>
    <tr>
      <td><strong>BEHAVIOR-100 Dataset</strong></td>
      <td>HDF5</td>
      <td>Datasets of human VR agents interacting with the simulation. First generation of BEHAVIOR replaced by BEHAVIOR 1K</td>
    </tr>
    <tr>
      <td><strong>D4RL</strong></td>
      <td>HDF5</td>
      <td>Franka Kitchen environment<br>containing several common household items: a microwave, a kettle,<br>an overhead light, cabinets, and an oven. The 4 subtasks are: <br>open the microwave, move the kettle, flip the light switch, and slide open the cabinet door,</td>
    </tr>
    <tr>
      <td><strong>ALFRED</strong></td>
      <td>JSON, Raw Images, MP4, PDDL</td>
      <td>Single room interactive visual dataset with high-level goal and low-level<br>natural language instructions for object and environment interaction.</td>
    </tr>
    <tr>
      <td><strong>VirtualHome</strong></td>
      <td>JSON</td>
      <td>Embodied household benchmark, activities in are represented by <em>programs</em> (sequence of actions) and <em>graphs</em> (definition of the environment where the activity takes place).</td>
    </tr>
  </tbody>
</table>
</details>

<details>
<summary><i>Full paper citations </i></summary>

- **BEHAVIOR-1k (2025 BEHAVIOR Challenge)**, BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation, 2022.12, Conference on Robot Learning. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2403.09227) [  ğŸŒ&nbsp;Website](https://behavior.stanford.edu/challenge/index.html) [ğŸ’¾ Dataset](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)
    
- **DexMimicGen**, DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning, 2024.10, ICRA 2025. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2410.24185) [  ğŸŒ&nbsp;Website](https://dexmimicgen.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/MimicGen/dexmimicgen_datasets/tree/main)
    
- **Lerobot (Meta-World+)**, Meta-World+: An Improved, Standardized, RL Benchmark, 2025.05. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2505.11289) [  ğŸŒ&nbsp;Website](https://meta-world.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/lerobot/metaworld_mt50)
    
- **ManiSkill 3**, A Unified Simulator for Physics-Based Robot Learning, 2024.10. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2410.00425) [  ğŸŒ&nbsp;Website](https://www.maniskill.ai/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/haosulab/ManiSkill_Demonstrations)
    
- **LIBERO**, LIBERO: Benchmarking Knowledge Transfer in Lifelong Robot Learning, 2023.06. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2306.03310) [  ğŸŒ&nbsp;Website](https://libero-project.github.io/datasets) [ğŸ’¾ Dataset](https://libero-project.github.io/datasets)
    
- **Habitat**, Habitat: A Platform for Embodied AI Research, 2019, ICCV. [ğŸ“„&nbsp;Paper (v2.0)](https://arxiv.org/abs/2106.14405) [ğŸ“„&nbsp;Paper (v3.0)](https://arxiv.org/abs/2310.13724) [  ğŸŒ&nbsp;Website](https://aihabitat.org/) [ğŸ’¾ Dataset](https://huggingface.co/ai-habitat)
    
- **MimicGen**, MimicGen: Language-guided Data Generation for Imitation Learning, 2023.10. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.17596) [  ğŸŒ&nbsp;Website](https://mimicgen.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/amandlek/mimicgen_datasets)
    
- **ManiSkill 2**, ManiSkill2: A Unified Benchmark for High-Level Robot Manipulation Tasks, 2023.02. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2302.04659) [  ğŸŒ&nbsp;Website](https://maniskill2.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/haosulab/ManiSkill2)
    
- **CALVIN Dataset**, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, 2021.12, IEEE Robotics and Automation Letters. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2112.03227) [  ğŸŒ&nbsp;Website](http://calvin.cs.uni-freiburg.de/) [ğŸ’¾ Dataset](https://github.com/mees/calvin/tree/main/dataset)
    
- **RLBench (Perceiver-Actor Dataset)**, Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation, 2022.09. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2209.05451) [  ğŸŒ&nbsp;Website](https://peract.github.io/) [ğŸ’¾ Dataset](https://huggingface.co/datasets/hqfang/rlbench-18-tasks)
    
- **Mobile Manipulation RoboTurk (MoMaRT)**, Learning Human-Guided Mobile Manipulation, 2021.12. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2112.05251) [  ğŸŒ&nbsp;Website](https://sites.google.com/view/il-for-mm/home) [ğŸ’¾ Dataset](https://sites.google.com/view/il-for-mm/datasets#h.ko0ilbky4y5u)
    
- **BEHAVIOR-100 Dataset**, BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments, 2021.08, CoRL 2021. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2108.03332) [  ğŸŒ&nbsp;Website](https://behavior.stanford.edu/behavior_100/overview.html) [ğŸ’¾ Dataset](https://behavior.stanford.edu/behavior_100/dataset.html)
    
- **D4RL**, D4RL: Datasets for Deep Data-Driven Reinforcement Learning, 2020.04, ICLR 2021. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2004.07219) [  ğŸŒ&nbsp;Website](https://sites.google.com/view/d4rl-anonymous/) [ğŸ’¾ Dataset](https://minari.farama.org/datasets/D4RL/kitchen/)
    
- **ALFRED**, ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks, 2019.12. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1912.01734) [  ğŸŒ&nbsp;Website](https://askforalfred.com/) [ğŸ’¾ Dataset](https://github.com/askforalfred/alfred/tree/master/data)
    
- **VirtualHome**, VirtualHome: Simulating Household Activities via a Generative and Interactive Model, 2018.06. [ğŸ“„&nbsp;Paper](http://virtual-home.org/paper/virtualhome.pdf) [  ğŸŒ&nbsp;Website](http://virtual-home.org/) [ğŸ’¾ Dataset](https://github.com/xavierpuigf/virtualhome/blob/master/virtualhome/dataset/README.md)
  </details>
---
## Real-world

| **NAME**            | YEAR | SIZE                                                                                 | MODALITIES                                                                                                                                                                                                        | ROBOTS                                                     | UNI / ORG                                                                                                              | Links                                                                                                                                                                                                                                |
| ------------------- | ---- | ------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **DROID**           | 2024 | ğŸ  564 scenes<br><br>ğŸ§© 86 <br>tasks                                                 | ğŸ–¼ï¸ full-HD stereo videos for all three cameras,<br><br>ğŸ¤– [RLDS](https://droid-dataset.github.io/droid/the-droid-dataset),<br><br>ğŸ—£ï¸ [Language  annotation (added in 2025)](https://huggingface.co/KarlP/droid) | Panda                                                      | 13 Universities (Stanford, <br>Berkeley, <br>Princeton, Washington <br>and [others](https://droid-dataset.github.io/)) | [ğŸŒ&nbsp;Website](https://droid-dataset.github.io/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2403.12945)<br>[ğŸ’¾&nbsp;Data](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing)                    |
| **Mobile ALOHA**    | 2024 | ğŸ§© 7 tasks                                                                           | ğŸ–¼ï¸ Three camera positions, cam_high, cam_left_wrist, cam_right_wrist <br><br>ğŸ¤– action, effort ,  pose <br>                                                                                                      | Mobile base<br>(two ViperX arms, two WidowX<br>for teleop. | Stanford                                                                                                               | [ğŸŒ&nbsp;Website](https://mobile-aloha.github.io/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2401.02117)<br>[ğŸ’¾&nbsp;Data](https://drive.google.com/drive/folders/1FP5eakcxQrsHyiWBRDsMRvUfSxeykiDc)                                  |
| **BridgeData V2**   | 2023 | ğŸ  24 envs.<br><br>ğŸ§© 13 skills                                                      | ğŸ–¼ï¸ Multiple camera views and depth data<br><br>ğŸ¤– robot arm trajectory<br><br>ğŸ—£ï¸ Natural language labels                                                                                                        | WidowX                                                     | UC Berkeley, Stanford, <br>Google <br>DeepMind, <br>CMU                                                                | [ğŸŒ&nbsp;Website](https://rail-berkeley.github.io/bridgedata/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2308.12952)<br>[ğŸ’¾&nbsp;Data](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/)                                  |
| **RoboSet**         | 2023 | ğŸ•¹ï¸ 30,050 trajectories<br><br>ğŸ§© 38 tasks                                           | ğŸ–¼ï¸ RGB cam<br><br>ğŸ—£ï¸ Language instructions <br><br>ğŸ¤– Proprio. (state, velocity)                                                                                                                                | Panda                                                      | Carnegie<br>Mellon <br>Uni.                                                                                            | [ğŸŒ&nbsp;Website](https://robopen.github.io/roboset/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2309.01918)<br>[ğŸ’¾&nbsp;Data](https://github.com/vikashplus/robohive/wiki/7.-Datasets)                                                |
| **Furniture Bench** | 2023 | ğŸ§© 8<br>furniture <br>tasks<br><br>ğŸ•¹ï¸ 5100<br>teleop.<br>demos.                     | ğŸ–¼ï¸  Wrist camera image, Front camera image <br><br>ğŸ¤– Robot state                                                                                                                                                | Panda                                                      | Korea <br>Advanced <br>Institute <br>of <br>Science <br>and <br>Technology                                             | [ğŸŒ&nbsp;Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2305.12821)<br>[ğŸ’¾&nbsp;Data](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) |
| **RH20T**           | 2023 | ğŸ•¹ï¸110,000 Robot <br>demos.<br><br>ğŸ•¹ï¸110,000 Human <br>demos.<br><br>ğŸ§©140<br>tasks | ğŸ–¼ï¸ RGB image, Depth image, Binocular IR images, Audio<br><br>ğŸ¤– Robot proprioception<br> <br> ğŸ¤ Fingertip tactile<br><br>ğŸ—£ï¸ Task language description                                                          | Flexiv,<br>UR5,<br>Panda,<br>Kuka                          | Shanghai AI Labs, <br>Beihang Uni., Shanghai <br>Jiao Tong Uni.<br>Sydney, <br>UESTC                                   | [ğŸŒ&nbsp;Website](https://rh20t.github.io/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2307.00595)<br>[ğŸ’¾&nbsp;Data](https://rh20t.github.io/#download)                                                                                |
| **Dobb-E**          | 2023 | ğŸ•¹ï¸ 13h of interactions<br><br>ğŸ  22 homes<br><br>ğŸ§©109 tasks                        | ğŸ–¼ï¸ RGB video, depth video <br> <br> ğŸ¤– Gripper 6D pose, gripper opening                                                                                                                                          | Stretch                                                    | New York Uni.                                                                                                          | [ğŸŒ&nbsp;Website](https://dobb-e.com/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2311.16098)<br> ğŸ’¾ [Data](https://github.com/notmahi/dobb-e/)                                                                                        |
| **MT-Opt**          | 2021 | ğŸ•¹ï¸ 800,000 episodes<br><br>ğŸ§© 12 real-world tasks                                   | ğŸ–¼ï¸ RGB image <br><br>ğŸ¤– Robot <br>action, state                                                                                                                                                                  | Kuka                                                       | Robotics at Google                                                                                                     | [ğŸŒ&nbsp;Website](https://karolhausman.github.io/mt-opt/) <br> [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2104.08212) <br> ğŸ’¾ [Data](https://www.tensorflow.org/datasets/catalog/mt_opt)                                                  |
| **RoboNet**         | 2019 | ğŸ•¹ï¸ 162,000 trajectories                                                             | ğŸ–¼ï¸ RGB image <br><br>ğŸ¤– Action sequences, end-effector pose, gripper state                                                                                                                                       | Sawyer, Panda,<br>Kuka,<br>Baxter, WidowX, Fetch           | UC Berkeley, Stanford Uni., U Pennsylvania, CMU                                                                        | [ğŸŒ&nbsp;Website](https://www.robonet.wiki/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1910.11215)<br>[ğŸ’¾&nbsp;Data](https://github.com/SudeepDasari/RoboNet/wiki/Getting-Started)                                                    |
| **BC-Z**            | 2021 | ğŸ•¹ï¸ 25,877 demos.<br><br>ğŸ§© 100 tasks                                                | ğŸ–¼ï¸ RGB image <br><br>ğŸ—£ï¸ Language instructions <br><br>ğŸ¤– Actions                                                                                                                                                | Google Robots (7-DOF arms)                                 | Google, <br>UC Berkeley                                                                                                | [ğŸŒ&nbsp;Website](https://sites.google.com/view/bc-z/home)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2202.02005)<br>[ğŸ’¾&nbsp;Data](https://www.kaggle.com/datasets/google/bc-z-robot)                                                 |
| **ManiWAV**         | 2024 | ğŸ§© 5 tasks                                                                           | ğŸ–¼ï¸ RGB <br>fisheye lens <br>Wrist cam<br><br>ğŸ¤– Camera pos<br><br>ğŸ¤ Microphone in griper (ear-in-hand)                                                                                                          | UR5                                                        | Stanford, Columbia, Toyota Research Institute                                                                          | [ğŸŒ&nbsp;Website](https://maniwav.github.io/)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2406.19464)<br>[ğŸ’¾&nbsp;Data](https://real.stanford.edu/maniwav/)                                                                             |
|                     |      |                                                                                      |                                                                                                                                                                                                                   |                                                            |                                                                                                                        |                                                                                                                                                                                                                                      |

<details>
  <summary><i>Additional notes</i></summary>

<table>
  <thead>
    <tr>
      <th>NAME</th>
      <th>DATA FORMAT</th>
      <th>NOTES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DROID</td>
      <td>RLDS</td>
      <td>One hardware setup across 13 institutions for data collection.</td>
    </tr>
    <tr>
      <td>Mobile ALOHA</td>
      <td>HDF5</td>
      <td>Bi-manual mobile manipulation. Co-training with existing static ALOHA datasets.</td>
    </tr>
    <tr> <td>ManiWAV</td> <td>MP4, wav, json</td> <td>Teleoperated dataset with microphones inside the compliant gripper for contact-rich manipulation. Moreover passively sensing the contact events and modes, or actively sensing the object  surface materials and states </td> </tr>
    <tr>-
      <td>BridgeData V2</td>
      <td>HDF5</td>
      <td>Teleoperation of the robot with a VR<br>controller.</td>
    </tr>
    <tr>
      <td>RoboSet</td>
      <td>HDF5</td>
      <td>A large-scale, real-world multi-task dataset for everyday household activities in kitchen scenes, collected via kinesthetic and teleoperated demonstrations.</td>
    </tr>
    <tr>
      <td>Furniture Bench</td>
      <td>Pickle format</td>
      <td>3D printed furniture tasks. Data collected with Oculus Quest 2 controller and a keyboard.</td>
    </tr>
    <tr>
      <td>RH20T</td>
      <td>Compressed tarball of MP4 and NumPy data</td>
      <td>Includes pairs of human and robot demonstrations for each task. Also contains single sentence task descriptions.</td>
    </tr>
    <tr>
      <td>Dobb-E</td>
      <td>Custom (includes RGB/depth videos and action annotations)</td>
      <td>An open-source framework using a low-cost tool ("The Stick") to collect the "Homes of New York (HoNY)" dataset for teaching robots household tasks via imitation learning.</td>
    </tr>
    <tr>
      <td>MT-Opt</td>
      <td>Offline RL dataset format</td>
      <td>Multi-robot collective learning system for data collection. Pick Place tasks with 7 Kukas.</td>
    </tr>
    <tr>
      <td>RoboNet</td>
      <td>TFRecord</td>
      <td>Open database for sharing robot experience from 7 different robot platforms and institutions.</td>
    </tr>
    <tr>
      <td>BC-Z</td>
      <td>RLDS (via TFDS)</td>
      <td>A large-scale imitation learning system for zero-shot and few-shot generalization to novel manipulation tasks not seen during training.</td>
    </tr>
  </tbody>
</table>
</details>

<details>
<summary><i>Full paper citations </i></summary>

- **DROID**, DROID: A Large-Scale Robot Learning Dataset with High-Quality Demonstrations, 2024.03. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2403.12945) [  ğŸŒ&nbsp;Website](https://droid-dataset.github.io/) [ğŸ’¾ Dataset](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing)
    
- **Mobile ALOHA**, Mobile ALOHA: A Low-Cost, Full-Body Mobile Robot for Daily Living, 2024.01. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2401.02117) [  ğŸŒ&nbsp;Website](https://mobile-aloha.github.io/) [ğŸ’¾ Dataset](https://drive.google.com/drive/folders/1FP5eakcxQrsHyiWBRDsMRvUfSxeykiDc)

- **ManiWAV**, ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data, 2024.06. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2406.19464) [ ğŸŒ&nbsp;Website](https://maniwav.github.io/) [ğŸ’¾ Dataset](https://real.stanford.edu/maniwav/) 

- **BridgeData V2**, BridgeData V2: A Large-Scale Dataset for Learning General-Purpose Robot Manipulation Policies, 2023.08. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2308.12952)  [ğŸŒ&nbsp;Website](https://rail-berkeley.github.io/bridgedata/) [ğŸ’¾ Dataset](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/)
    
- **RoboSet**, RoboSet: A Large-Scale Dataset for Robot Manipulation with Diverse Objects and Actions, 2023.09. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2309.01918) [  ğŸŒ&nbsp;Website](https://robopen.github.io/roboset/) [ğŸ’¾ Dataset](https://github.com/vikashplus/robohive/wiki/7.-Datasets)
    
- **Furniture Bench**, Furniture Bench: A Benchmark for Assembling Furniture with a Robot, 2023.05. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2305.12821) [  ğŸŒ&nbsp;Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) [ğŸ’¾ Dataset](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)
    
- **RH20T**, RH20T: A Benchmark for Robot Hand Dexterous Manipulation, 2023.07. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2307.00595) [  ğŸŒ&nbsp;Website](https://rh20t.github.io/) [ğŸ’¾ Dataset](https://rh20t.github.io/#download)
    
- **Dobb-E**, Dobb-E: A Dataset for Robot Manipulation with Unseen Objects, 2023.11. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2311.16098) [  ğŸŒ&nbsp;Website](https://dobb-e.com/) [ğŸ’¾ Dataset](https://github.com/notmahi/dobb-e/)
    
- **MT-Opt**, MT-Opt: A Multi-Task, Multi-Modal Robot Learning Framework, 2021.04. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2104.08212) [  ğŸŒ&nbsp;Website](https://karolhausman.github.io/mt-opt/) [ğŸ’¾ Dataset](https://www.tensorflow.org/datasets/catalog/mt_opt)
    
- **RoboNet**, RoboNet: A Dataset for Robot Grasping and Manipulation, 2019.10. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1910.11215) [  ğŸŒ&nbsp;Website](https://www.robonet.wiki/) [ğŸ’¾ Dataset](https://github.com/SudeepDasari/RoboNet/wiki/Getting-Started)
    
- **BC-Z**, BC-Z: A Large-Scale Robot Learning Benchmark for Dexterous Manipulation, 2022.02. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2202.02005) [  ğŸŒ&nbsp;Website](https://sites.google.com/view/bc-z/home) [ğŸ’¾ Dataset](https://www.kaggle.com/datasets/google/bc-z-robot)

</details>

---

## Combined (Sim + Real)

| **NAME**      | YEAR | SIZE                                                                                                                         | MODALITIES                                                                                                                                                                | ROBOTS | UNI / ORG             | Links                                                                                                                                                                                                                                    |
| ------------- | ---- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **RoboTurk**  | 2019 | ***Real*** <br>ğŸ•¹ï¸ 2144 demos.<br><br>***Sim*** <br>ğŸ•¹ï¸ 1070 Sawyer<br>PickPlace<br>ğŸ•¹ï¸1147 <br>Sawyer<br>NutAssembly demos. | ***Real***  <br>ğŸ–¼ï¸ Multiple camera views and depth data<br><br>ğŸ¤– control data from the user and joint data from the robot<br> <br> ***Sim***<br> ğŸ¤– Robot state, action | Sawyer | Stanford              |   [ğŸŒ&nbsp;Website](https://roboturk.stanford.edu/index.html)<br>[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1811.02790)<br>***Real*** <br>[ğŸ’¾&nbsp;Data](https://roboturk.stanford.edu/dataset_real.html)<br>***Sim*** <br>[ğŸ’¾&nbsp;Data](https://roboturk.stanford.edu/dataset_sim.html) |
| **RoboMimic** | 2021 | ğŸ§© 5 tasks<br><br>ğŸ•¹ï¸ 500 human generated  trajectories<br><br>ğŸ•¹ï¸5400 machine generated trajectories<br>                    | ğŸ¤– Robot state, action                                                                                                                                                    | Franka | Stanford,<br>UTAustin |  [ğŸŒ&nbsp;Website](https://robomimic.github.io/) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2108.03298)<br>[ğŸ’¾&nbsp;Data](https://robomimic.github.io/docs/datasets/robomimic_v0.1.html)                                                                         |

<details>
  <summary><i>Additional notes</i></summary>

<table>
  <thead>
    <tr>
      <th><strong>NAME</strong></th>
      <th>DATA FORMAT</th>
      <th>NOTES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>RoboTurk</strong></td>
      <td>HDF5, MP4</td>
      <td>Real datasets contains laundry layout, tower creation, and object search tasks done (54 users).</td>
    </tr>
    <tr>
      <td><strong>RoboMimic</strong></td>
      <td>HDF5</td>
      <td>Dataset contains very simple tasks Lift, PickandPlace. Data collected using the RoboTurk platform.</td>
    </tr>
  </tbody>
</table>
</details>

<details>
<summary><i>Full paper citations </i></summary>

- **RoboTurk**, RoboTurk: A Crowd-Sourcing Platform for High-Quality Robot Data, 2018.11, CoRL 2018. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1811.02790) [  ğŸŒ&nbsp;Website](https://roboturk.stanford.edu/index.html) [ğŸ’¾ Dataset](https://roboturk.stanford.edu/dataset_real.html)
    
- **RoboMimic**, RoboMimic: A Dataset and Benchmark for Vision-Based Robot Manipulation, 2021.08. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2108.03298) [  ğŸŒ&nbsp;Website](https://robomimic.github.io/) [ğŸ’¾ Dataset](https://robomimic.github.io/docs/datasets/robomimic_v0.1.html)
</details>

---

## Dataset Collections


| **NAME**                     | YEAR | SIZE                                               | MODALITIES                                                                                                  | ROBOTS                                                                    | UNI / ORG                                                                                                            | Links                                                                                                                                                                                                                                                                                      |
| ---------------------------- | ---- | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Open-X Embodiment**        | 2023 | 71 **datasets** (last checked 15.9.2025)           | ğŸ–¼ï¸ RGB, depth cameras <br><br>ğŸ—£ï¸ Language annotations<br><br>ğŸ¤– Robot state, action                       | Franka, Spot, Hello Stretch, UR5, Sawyer, ViperX,<br>...                  | Arizona State Uni.,<br>Carnegie Mellon Uni. ,Columbia Uni.,<br>ETH ZÃ¼rich, Google DeepMind, and many [other]<br><br> |   [ğŸŒ&nbsp;Website](https://robotics-transformer-x.github.io/) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.08864) [ğŸ’¾&nbsp;Data](https://robotics-transformer-x.github.io/) ğŸ“Š[Dataset overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0)        |
| **ARIO (All Robots in One)** | 2024 | ğŸ§©321,064 tasks <br>($\approx$ 3 million episodes) | ğŸ–¼ï¸ RGBD,<br>Point cloud<br><br>ğŸ—£ï¸ Language annotations,<br>Sound<br><br>ğŸ¤– Proprioception<br><br>ğŸ¤ Touch | Franka, Cobot<br>Magic, Cloud Ginger, UR5, Sawyer,<br>Konova Gen3,<br>... | Peng Cheng Laboratory, Southern U Science and Technology, Sun Yat-sen Uni., ARIO Alliance                            |   [ğŸŒ&nbsp;Website](https://imaei.github.io/project_pages/ario/) [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2408.10899) [ğŸ’¾&nbsp;Dataset](https://imaei.github.io/project_pages/ario/) ğŸ“œ[Full Task List](https://imaei.github.io/project_pages/ario/static/agilex%20cobot-magic%20data%20collection%20tasks.pdf) |


<details>

<summary><i>Additional notes</i></summary>

<table>

<thead>

<tr>

<th><strong>NAME</strong></th>

<th>DATA FORMAT</th>

<th>NOTES</th>

</tr>

</thead>

<tbody>

<tr>

<td><strong>Open-X Embodiment</strong></td>

<td>RLDS</td>

<td>Information about exact modalities and robots in <a href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0">google sheet overview</a>. To get SOTA results not all datasets are used (filtering is required). Open-X provides unified download and format of datasets.</td>

</tr>

<tr>

<td><strong>ARIO (All Robots in One)</strong></td>

<td>New standard, unified data format</td>

<td>Large part of dataset is recorded on Cobot Magic platform with long-horizon, bimanual manipulation tasks and also <strong>human-robot collaboration.</strong></td>

</tr>

</tbody>

</table>

</details>

<details>

<summary><i>Full paper citations </i></summary>

- **Open-X Embodiment**, Open-X Embodiment: A New Benchmark for Large-Scale Robot Learning, 2023.10. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.08864) [  ğŸŒ&nbsp;Website](https://robotics-transformer-x.github.io/) [ğŸ’¾ Dataset](https://robotics-transformer-x.github.io/)
- **ARIO (All Robots in One)**, All Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents, 2024. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2408.10899) [  ğŸŒ&nbsp;Website](https://imaei.github.io/project_pages/ario/) [ğŸ’¾ Dataset](https://imaei.github.io/project_pages/ario/)

</details>

---
## Benchmarks

## Simulated

| NAME            | YEAR | TASKS                                                                                                          | SIMULATOR   | ROBOTS                      | UNI <br>/ ORG               | Links                                                                                                                                                                  |
| --------------- | ---- | -------------------------------------------------------------------------------------------------------------- | ----------- | --------------------------- | --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Habitat 3.0** | 2024 | [![Demo](/images/habitat.png)](https://github.com/facebookresearch/habitat-lab/blob/main/res/img/habitat3.gif) | Habitat     | Virtual<br>human<br>agents  | Meta AI                     | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.13724) [ ğŸŒ&nbsp;Website](https://aihabitat.org/habitat3/) [ğŸ› ï¸ Code](https://github.com/facebookresearch/habitat-lab/)     |
| **LIBERO**      | 2023 | [![Demo](/images/libero.png)](/videos/libero.gif)                                                              | RoboSuite   | Panda                       | UT Austin, Tsinghua Univers | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2306.03310) [  ğŸŒ&nbsp;Website](https://libero-project.github.io/datasets) [ğŸ› ï¸ Code](https://github.com/Lifelong-Robot-Learning/LIBERO) |
| **BEHAVIOR-1K** | 2022 | [![Demo](/images/behavior.png)](/videos/behavior.gif)                                                          | iGibson 2.0 | Panda, Fetch, TurtleBot     | Stanford                    | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2403.09227) [  ğŸŒ&nbsp;Website](https://behavior.stanford.edu/) [ğŸ› ï¸ Code](https://github.com/StanfordVL/BEHAVIOR-1K)                       |
| **CALVIN**      | 2022 | [![Demo](/images/calvin.png)](/videos/calvin.gif)                                                              | PyBullet    | Panda                       | TU Dresden                  | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2112.03227) [ ğŸŒ&nbsp;Website](http://calvin.cs.uni-freiburg.de/) [ğŸ› ï¸ Code](https://github.com/mees/calvin)                     |
| **ALFRED**      | 2020 | [![Demo](/images/alfred.png)](/videos/alfred.gif)                                                              | AI2-THOR    | Virtual agents (AI2-THOR)   | Allen Institute for AI      | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1912.01734) [ ğŸŒ&nbsp;Website](https://askforalfred.com/) [ğŸ› ï¸ Code](https://github.com/askforalfred/alfred)                                  |
| **RLBench**     | 2020 | [![Demo](/images/rlbench.png)](/videos/rlbench.gif)                                                            | PyRep       | Panda                       | Imperial College London     | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1909.12271) [ ğŸŒ&nbsp;Website](https://sites.google.com/view/rlbench) [ğŸ› ï¸ Code](https://github.com/stepjam/RLBench)             |
| **Meta-World**  | 2019 | [![Demo](/images/metaworld.png)](/videos/metaworld.gif)                                                        | MuJoCo      | Sawyer                      | UC Berkeley                 | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1910.10897) [ ğŸŒ&nbsp;Website](https://meta-world.github.io/) [ğŸ› ï¸ Code](https://github.com/Farama-Foundation/Metaworld)         |
| **VirtualHome** | 2018 | [![Demo](/images/virtualhome.png)](/videos/virtualhome.gif)                                                    | Unity3D     | Virtual <br>human<br>agents | MIT, Stanford               | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1806.07011) [ ğŸŒ&nbsp;Website](http://virtual-home.org/) [ğŸ› ï¸ Code](https://github.com/xavierpuigf/virtualhome)                                    |




<details>
<summary><i>Full paper citations </i></summary>

- **Habitat 3.0**, Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots, 2023.10. [ğŸ“„ Paper](https://arxiv.org/abs/2310.13724) [ğŸŒ Website](https://aihabitat.org/habitat3/) [ğŸ› ï¸ Code](https://github.com/facebookresearch/habitat-lab)

- **LIBERO**, LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning, 2023.06. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2306.03310) [  ğŸŒ&nbsp;Website](https://libero-project.github.io/datasets) [ğŸ› ï¸ Code](https://libero-project.github.io/datasets)
    
- **BEHAVIOR-1K**, BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation, 2024.03. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2403.09227) [  ğŸŒ&nbsp;Website](https://behavior.stanford.edu/challenge/index.html) [ğŸ› ï¸ Code](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)
    
- **CALVIN**, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, 2021.12, IEEE Robotics and Automation Letters. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2112.03227) [  ğŸŒ&nbsp;Website](http://calvin.cs.uni-freiburg.de/) [ğŸ› ï¸ Code](https://github.com/mees/calvin)
    
- **Habitat 2.0**, Habitat 2.0: Training Home Assistants to Rearrange their Habitat, 2021.06, NeurIPS 2021. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2106.14405) [  ğŸŒ&nbsp;Website](https://aihabitat.org/) [ğŸ› ï¸ Code](https://github.com/facebookresearch/habitat-lab)
    
- **ALFRED**, ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks, 2019.12, CVPR 2020. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1912.01734) [  ğŸŒ&nbsp;Website](https://askforalfred.com/) [ğŸ› ï¸ Code](https://github.com/askforalfred/alfred)
    
- **RLBench**, The Robot Learning Benchmark & Learning Environment, 2019.09, CoRL 2019. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1909.12271) [  ğŸŒ&nbsp;Website](https://sites.google.com/view/rlbench) [ğŸ› ï¸ Code](https://github.com/stepjam/RLBench)
    
- **BEHAVIOR-100**, BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments, 2021.08, CoRL 2021. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2108.03332) [  ğŸŒ&nbsp;Website](https://behavior.stanford.edu/behavior_100/overview.html) [ğŸ› ï¸ Code](https://behavior.stanford.edu/behavior_100/dataset.html)
    
- **Meta-World**, Meta-World: A Benchmark for Evaluating Multi-Task and Meta-Learning in Robotics, 2019.10, ICRA 2020. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/1910.10897) [  ğŸŒ&nbsp;Website](https://meta-world.github.io/) [ğŸ› ï¸ Code](https://github.com/rlworkgroup/metaworld)
    
- **VirtualHome**, VirtualHome: Simulating Household Activities via Programs, 2018.06, CVPR 2018. [ğŸ“„&nbsp;Paper](http://virtual-home.org/paper/virtualhome.pdf) [  ğŸŒ&nbsp;Website](http://virtual-home.org/) [ğŸ› ï¸ Code](https://github.com/xavierpuigf/virtualhome)

</details>

---
## Real life


| NAME                                                              | YEAR | TASK TYPE                                                         | PLATFORM<br>/ SET UP                       | ROBOTS | UNI / ORG                                                                                                      | Links                                                                                                                                                                                            |
| ----------------------------------------------------------------- | ---- | ----------------------------------------------------------------- | ------------------------------------------ | ------ | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **RoboArena**                                                     | 2025 | [![Demo](/images/roboarena.png)](/videos/roboarena.gif)           | DROID                                      | Panda  | UC Berkeley, <br>Stanford, <br>UWashington, <br>UMontreal, <br>NVIDIA, <br>UPenn, <br>UT Austin, <br>Yonsei U. | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2506.18123)<br>[ ğŸŒ&nbsp;Website](https://robo-arena.github.io/)<br>[ğŸ› ï¸Code](https://github.com/robo-arena/roboarena)                                     |
| **CT benchmark**                                                  | 2024 | [![Demo](/images/ct.png)](/videos/ct.gif)                         | 3D printed<br>city landscape model         | Kuka   | CEMMPRE, ARISE, University of<br>Coimbra                                                                       | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2402.00708)<br>[ğŸ› ï¸Code](https://github.com/Robotics-and-AI/collaborative-tasks-benchmark)                                                                 |
| **FMB**<br>(**F**unctional <br>**M**anipulation<br>**B**enchmark) | 2024 | [![Demo](/images/fmb.png)](/videos/fmb.gif)                       | 3D printed basic shape<br>objects          | Panda  | UC Berkeley                                                                                                    | [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2401.08553)<br>[ ğŸŒ&nbsp;Website](https://functional-manipulation-benchmark.github.io/)<br>[ğŸ› ï¸Code](https://github.com/rail-berkeley/fmb)                 |
| **FurnitureBench**                                                | 2023 | [![Demo](/images/furniturebench.png)](/videos/furniturebench.gif) | RealSense cameras and 3D printed furniture | Panda  | UC Berkeley,<br>KAIST                                                                                          | [ğŸŒ&nbsp;Website](https://clvrai.github.io/furniture-bench/)<br>[ğŸ› ï¸Code](https://clvrai.github.io/furniture-bench/)                                                                             |
| **Robothon Task Board**                                           | 2023 |                                                                   | Internet connected task box                | Any    | Modular Motions                                                                                                | [ğŸŒ&nbsp;Website](https://www.modularmotions.com/store/p4/iot-task-board-v2023.html)                                                                                                             |
| **NIST (Assembly<br>Task Boards)**                                | 2020 |                                                                   | Custom made task boards                    | Any    | NIST                                                                                                           | [ğŸ“„&nbsp;Paper](https://pubmed.ncbi.nlm.nih.gov/33029555/)<br>[ ğŸŒ&nbsp;Website](https://www.nist.gov/el/intelligent-systems-division-73500/robotic-grasping-and-manipulation-assembly/assembly) |



<details>
<summary><i>Full paper citations </i></summary>

- **CT Benchmark**, Benchmarking human-robot collaborative assembly tasks, 2024.[ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2402.00708) [ğŸ› ï¸ Code](https://github.com/Robotics-and-AI/collaborative-tasks-benchmark)
- **FurnitureBench**, Furniture Bench: A Benchmark for Assembling Furniture with a Robot, 2023.05. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2305.12821) [  ğŸŒ&nbsp;Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) [ğŸ› ï¸ Code](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)
- **RoboArena**, Distributed Real-World Evaluation of Generalist Robot Policies, 2025.06. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2506.18123) [  ğŸŒ&nbsp;Website](https://robo-arena.github.io/)
- **FMB**, J. Luo, C. Xu, F. Liu, L. Tan, Z. Lin, J. Wu, P. Abbeel, and S. Levine. FMB: A functional manipulation benchmark for generalizable robotic learning. _arXiv preprint arXiv:2401.08553_, 2024. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2401.08553) [  ğŸŒ&nbsp;Website](https://functional-manipulation-benchmark.github.io/)
</details>

---
# Physics-Based Simulation Frameworks

| Framework                                                                                            | Associated Benchmarks & Environments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **[NVIDIA Isaac Sim](https://developer.nvidia.com/isaac/sim)**                                       | [BEHAVIOUR-1K](https://behavior.stanford.edu/); [ARNOLD](https://arnold-benchmark.github.io/); [Isaac Lab ](https://isaac-sim.github.io/IsaacLab/main/index.html)[(ORBIT)](https://isaac-orbit.github.io/); [Isaac Lab Mimic](https://isaac-sim.github.io/IsaacLab/main/source/overview/teleop_imitation.html)                                                                                                                                                                                                                                                                                                                                                                                                              |
| [**NVIDIA Isaac Gym**](https://developer.nvidia.com/isaac-gym)                                       | [Furniture Bench](https://clvrai.github.io/furniture-bench/); [AutoMate](https://bingjietang718.github.io/automate/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **[MuJoCo](https://github.com/google-deepmind/mujoco)**                                              | [Robosuite](https://robosuite.ai/) ([Robomimic](https://robomimic.github.io/), [LIBERO](https://libero-project.github.io/main.html), [MuBle](https://michaal94.github.io/MuBlE/), [CompoSuite](https://github.com/Lifelong-ML/CompoSuite), [RoboCasa](https://robocasa.ai/)); [Meta-World ](https://meta-world.github.io/)([Continual World](https://sites.google.com/view/continualworld)); [MuJoBan, MuJoXO, MuJoGo](https://github.com/google-deepmind/deepmind-research/tree/master/physics_planning_games);  [ALOHA sim](https://github.com/google-deepmind/mujoco_menagerie/tree/main/aloha/), [VLABench](https://vlabench.github.io/); [Gymnasium-Robotics](https://github.com/Farama-Foundation/Gymnasium-Robotics) |
| **[SAPIEN](https://sapien.ucsd.edu/)**                                                               | [ManiSkill 3](https://www.maniskill.ai/); [ManiSkill 2](https://maniskill2.github.io/) ( [SimplerEnv](https://simpler-env.github.io/), [ClevrSkills](https://github.com/Qualcomm-AI-research/ClevrSkills), [VLATest](https://github.com/ma-labo/VLATest))                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **[Unity](https://unity.com/)**                                                                      | [AI2-THOR](https://ai2thor.allenai.org/) ([ManipulaTHOR](https://prior.allenai.org/projects/manipulathor), [ProcTHOR](https://procthor.allenai.org/), [ALFRED](https://askforalfred.com/))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| **[iGibson 2.0](https://svl.stanford.edu/igibson/)**                                                 | [BEHAVIOUR-100](https://behavior.stanford.edu/behavior_100/overview.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **[PyBullet](https://pybullet.org/wordpress/)**                                                      | [CausalWorld](https://sites.google.com/view/causal-world/home), [Habitat-Sim](https://github.com/facebookresearch/habitat-sim), [Calvin](http://calvin.cs.uni-freiburg.de/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| [**CoppeliaSim** ](https://www.coppeliarobotics.com/)**([PyRep](https://github.com/stepjam/PyRep))** | [RLBench](https://sites.google.com/view/rlbench) ([AGNOSTOS](https://jiaming-zhou.github.io/AGNOSTOS/))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [**Webots**](https://cyberbotics.com/)                                                               | [deepbots](https://github.com/aidudezzz/deepbots)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| [**Drake**](https://drake.mit.edu/)                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| [**Genesis**](https://genesis-embodied-ai.github.io/)                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Text-Oriented**                                                                                    | [TextWorld](https://github.com/Microsoft/TextWorld); [ALFWorld](https://alfworld.github.io/); [Tales](https://microsoft.github.io/tale-suite/); [WebShop](https://webshop-pnlp.github.io/); [PCA-Bench](https://arxiv.org/pdf/2310.02071); [PlanBench](https://github.com/karthikv792/LLMs-Planning)                                                                                                                                                                                                                                                                                                                                                                                                                        |

---
# Models

- **RoboPen**, RoboAgent: Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking, 2023.09. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2309.01918) [  ğŸŒ&nbsp;Website](https://robopen.github.io/) [ğŸ› ï¸ Code](https://robopen.github.io/)
    
- **Octo**, Octo: An Open-Source Generalist Robot Policy, 2024.05. [ğŸ“„&nbsp;Paper](https://www.semanticscholar.org/paper/1d2753d74025e7a71594506623be81f18b073adb) [ğŸ› ï¸ Code](https://github.com/octo-models/octo)
    
- **Hi Robot**, Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models, 2025.02. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2502.19417) [  ğŸŒ&nbsp;Website](https://www.themoonlight.io/en/review/hi-robot-open-ended-instruction-following-with-hierarchical-vision-language-action-models)
    
- **Do As I Can, Not As I Say (SayCan)**, Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, 2022.04. [ğŸ“„&nbsp;Paper](https://research.google/pubs/do-as-i-can-not-as-i-say-grounding-language-in-robotic-affordances/) [  ğŸŒ&nbsp;Website](https://say-can.github.io/)
    
- **Gemini Robotics**, Gemini Robotics: Bringing AI into the Physical World, 2025.03. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2503.20020) [  ğŸŒ&nbsp;Website](https://deepmind.google/models/gemini-robotics/)
    
- **RDT-1B**, RDT-1B: A Diffusion Foundation Model for Bimanual Manipulation, 2024.10. [ğŸ“„&nbsp;Paper](https://www.alphaxiv.org/overview/2410.07864v1) [  ğŸŒ&nbsp;Website](https://rdt-robotics.github.io/rdt-robotics/) [ğŸ› ï¸ Code](https://github.com/thu-ml/RoboticsDiffusionTransformer)
    
- **RT-X**, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, 2023.10. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2310.08864) [  ğŸŒ&nbsp;Website](https://robotics-transformer-x.github.io/) [ğŸ› ï¸ Code](https://github.com/kyegomez/RT-X)
    
- **pi0**, Ï€0: A Foundation Model for Robotics with Sergey Levine, 2025.02. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2410.24164) [  ğŸŒ&nbsp;Website](https://physical-intelligence.com/) [ğŸ› ï¸ Code](https://github.com/Physical-Intelligence/openpi)
    
- **OpenVLA**, OpenVLA: An Open-Source Vision-Language-Action Model, 2025.05. [ğŸ“„&nbsp;Paper](https://proceedings.mlr.press/v270/kim25c.html) [  ğŸŒ&nbsp;Website](https://openvla.github.io/) [ğŸ› ï¸ Code](https://openvla.github.io/) [ğŸ› ï¸ Code](https://github.com/openvla/openvla)
    
- **MolmoAct**, MolmoAct: Action Reasoning Models that can Reason in Space, 2025.08. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2508.07917) [  ğŸŒ&nbsp;Website](https://allenai.org/blog/molmoact) [ğŸ› ï¸ Code](https://github.com/allenai/molmoact)
	
- **ManiFlow**, A General Robot Manipulation Policy via Consistency Flow Training, 2025.09. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2509.01819) [  ğŸŒ&nbsp;Website](https://maniflow-policy.github.io/)
    
- **AutoMate**, Specialist and Generalist Assembly Policies over Diverse Geometries, 2024.07. [ğŸ“„&nbsp;Paper](https://arxiv.org/abs/2407.08028) [  ğŸŒ&nbsp;Website](https://bingjietang718.github.io/automate/) [ğŸ› ï¸ Code](https://github.com/isaac-sim/IsaacGymEnvs/blob/automate/docs/automate.md)

---
# Papers to include
- datasets:
    - Papers without code:
		**[https://momagen-rss.github.io/](https://momagen-rss.github.io/)** (16.9.2025 no code yet)
	-  Datasets website unavailable till 26 October
		- [Songling_datasets](https://openi.pcl.ac.cn/ARIO/Songling_datasets), Songling Cobot Magic hardware platform, 2 tasks, 70 episodes, 3 RGBDs, 2 arms  
		- [SeaWave](https://openi.pcl.ac.cn/ARIO/SeaWave), Dataa Robotâ€™s Cloud Ginger XR-1, 3 tasks and about 800 episodes.  
		- [PCL_CollectInReal](https://openi.pcl.ac.cn/ARIO/PCL_CollectInReal), Cobot Magic hardware platform, 48 tasks, 2414 episodes, 3 RGBDs, 2 arms
	

- benchmarks:
	- K. Kimble, K. Van Wyk, J. Falco, E. Messina, Y. Sun, M. Shibata, W. Uemura, and Y. Yokokohji. Benchmarking Protocols for Evaluating Small Parts Robotic Assembly Systems. _IEEE Robotics and Automation Letters_, 5(2), 2020. [ğŸ“„&nbsp;Paper](https://pubmed.ncbi.nlm.nih.gov/33029555/) https://www.uml.edu/research/nerve/nist-assembly-task-board-form.aspx
	- https://arxiv.org/abs/2103.05140
	- https://www.nist.gov/el/intelligent-systems-division-73500/robotic-grasping-and-manipulation-assembly/assembly
	- https://www.modularmotions.com/store/p4/iot-task-board-v2023.html


