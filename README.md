# Datasets

## Simulated

| NAME                                                                            | Links                                                                                                                                                                                                                                                                                                                                                                                       | YEAR | SIZE                                                                                                                                                                      | MODALITIES                                                                                                                                                                 | UNIVERSITY / ORG                                                                         | DATA FORMAT                            | ROBOTS / ENVIRONMENT                                                                                                         | NOTES                                                                                                                                                                               |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| BEHAVIOR-1k  (2025 BEHAVIOR Challenge)                                          | üåê [Project Website](https://behavior.stanford.edu/challenge/index.html) üìÑ [Paper](https://arxiv.org/abs/2403.09227) üíæ [Dataset Download](https://huggingface.co/datasets/behavior-1k/2025-challenge-demos)                                                                                                                                                                               | 2025 | üß© 50 full-length household tasks <br>üïπÔ∏è 10,000 teleoperated demonstrations                                                                                              | üñºÔ∏è  RGB, depth, and instance segmentation images<br>ü§ñ Robot state, action, task information                                                                              | Stanford                                                                                 | LeRobot format (parquet)               | Galaxea R1 robot                                                                                                             | Datasets is human-grounded benchmark in house-scale scenes part of the [2025 EAI Challenge](https://foundation-models-meet-embodied-agents.github.io/eai_challenge/)                |
| Lerobot (Meta-World+)                                                           | üåê [Project Website](https://meta-world.github.io/) üíæ [Dataset Download](https://huggingface.co/datasets/lerobot/metaworld_mt50)                                                                                                                                                                                                                                                           | 2025 | üß© 50 tasks                                                                                                                                                               | üñºÔ∏è RGB-D image, <br> ü§ñ Robot action,  Environment state                                                                                                                  | Farma foundation, LeRobot                                                                | LeRobot format (parquet)               | Sawyer                                                                                                                       | Generated using expert policies.                                                                                                                                                    |
| ManiSkill 3 (Datasets creation inprogress as of 1.9.2025)                       | üåê [Project Website](https://www.maniskill.ai/) üìÑ [Paper](https://arxiv.org/abs/2410.00425)  üíæ [Dataset Download](https://huggingface.co/datasets/haosulab/ManiSkill_Demonstrations)                                                                                                                                                                                                      | 2024 | üè† 16 environments                                                                                                                                                        | ü§ñ Demonstrations (motion planning, teleop) <br> üñºÔ∏è RGB-D, point clouds                                                                                                   | UC San Diego (SU Lab)                                                                    | HDF5, JSON                             | [Unitree G1/H1, Franka Panda, UR10e, Allegro Hand, XArm, etc.](https://maniskill.readthedocs.io/en/latest/robots/index.html) | In development; most environments are missing demos.                                                                                                                                |
| LIBERO                                                                          | üåê [Project Website](https://libero-project.github.io/datasets)  üìÑ [Paper](https://arxiv.org/abs/2306.03310) üíæ [Dataset Download](https://libero-project.github.io/datasets)                                                                                                                                                                                                              | 2023 | üß© 130 tasks                                                                                                                                                              | üñºÔ∏è RGB images from workspace and wrist cameras <br> ü§ñ Proprioception <br> üó£Ô∏è Language task specifications <br> üìù PDDL scene descriptions                               | UT Austin, Tsinghua University                                                           | HDF5                                   | Franka Panda                                                                                                                 | Human-teleoperated demonstrations.                                                                                                                                                  |
| Habitat                                                                         | (2.0)<br>üåê [Project Website](https://aihabitat.org/)üìÑ [Paper](https://arxiv.org/abs/2106.14405)<br>(3.0)<br>üåê [Project Website](https://aihabitat.org/habitat3/) üìÑ [Paper](https://arxiv.org/abs/2310.13724)  üíæ [Dataset Download Hugging Face](https://huggingface.co/ai-habitat)<br>üíæ [Dataset Download GIT](https://github.com/facebookresearch/habitat-lab/blob/main/DATASETS.md) | 2023 | Object and point goal navigation, Embodied Question Answering, Visual Language Navigation, Rearrange Pick                                                                 | üó£Ô∏è Scene description and language annotation (EQA, VLN)                                                                                                                   | Facebook AI Research, Georgia Tech, UC Berkeley, Simon Fraser University, Intel Research | JSON                                   | Fetch robot, Virtual Human Agets                                                                                             | Habitat 2.0 contains Scans and CAD Replicas (with simulation framework) of interiors. Embodied datasets mainly came with Habitat 3.0 datasets. Relevant Docs for Sim in Habitat 2.0 |
| ManiSkill 2                                                                     | üåê [Project Website](https://maniskill2.github.io/)üìÑ [Paper](https://arxiv.org/abs/2302.04659)üíæ [Dataset Download](https://huggingface.co/datasets/haosulab/ManiSkill2)                                                                                                                                                                                                                   | 2023 | üß© 20 tasks                                                                                                                                                               | ü§ñ Demonstrations (scene and robot states) <br> üñºÔ∏è Replay tools (point cloud, RGB-D)                                                                                      | UC San Diego, Tsinghua University                                                        | HDF5, JSON                             | Franka Panda                                                                                                                 | Replaced by ManiSkill 3 (still in beta).                                                                                                                                            |
| CALVIN (**C**omposing **A**ctions from **L**anguage and **Vi**sio**n**) Dataset | üåê [Project Website](http://calvin.cs.uni-freiburg.de/)  üìÑ [Paper](https://arxiv.org/abs/2112.03227)  üíæ [Dataset Download](https://github.com/mees/calvin/tree/main/dataset)                                                                                                                                                                                                              | 2022 | üè† 4 environments<br>(24 hours of teleoperated ‚Äúplay‚Äù)                                                                                                                    | üñºÔ∏è RGB-D images from both static and gripper-mounted cameras<br> üó£Ô∏è Language Annotations<br> ü§ñ Action (tool center point), State Observation<br> ü§è Tactile image input | University of Freiburg                                                                   | NumPy files                            | Franka Panda                                                                                                                 | Long horizon in terms of sequence length. Data colected with HTC Vive VR headset.                                                                                                   |
| RLBench (Perceiver-Actor Dataset)                                               | ***Perceiver-Actor*** <br>üåê [Project Website](https://peract.github.io/)üìÑ [Paper](https://arxiv.org/abs/2209.05451)<br>***RLBench***<br>üåê [Project Website](https://sites.google.com/view/rlbench)üìÑ [Paper](https://arxiv.org/abs/1909.12271)                                                                                                                                           | 2022 | RLBench contains a dataset generator, for example pregenerated 18 tasks in Perceiver-actor üíæ [Dataset Download](https://huggingface.co/datasets/hqfang/rlbench-18-tasks) | üñºÔ∏è Multyple RBG, depth and mask camera views <br>ü§ñ Episode variation description                                                                                         | University of Washington, NVIDIA                                                         | PNG, Pickle format                     | Franka                                                                                                                       | Generated simulation datasets using the RLBench datasets generation utility (using template policies)                                                                               |
| BEHAVIOR-100 Dataset                                                            | üåê [Project Website](https://behavior.stanford.edu/behavior_100/overview.html)  üìÑ [Paper](https://arxiv.org/abs/2108.03332)  <br>üíæ [Dataset Download](https://behavior.stanford.edu/behavior_100/dataset.html)                                                                                                                                                                            | 2021 | üïπÔ∏è 500 VR demonstrations                                                                                                                                                 | üñºÔ∏è semantic segmentation, instance segmentation, RGB, depth, highlight<br>ü§ñ proprioception, agent action                                                                 | Stanford                                                                                 | HDF5                                   | VR agent                                                                                                                     | First generation of BEHAVIOR replaced by BEHAVIOR 1K                                                                                                                                |
| ALFRED                                                                          | üåê [Project Website](https://askforalfred.com/)  üìÑ [Paper](https://arxiv.org/abs/1912.01734)üíæ [Dataset Download](https://github.com/askforalfred/alfred/tree/master/data)                                                                                                                                                                                                                 | 2020 | üïπÔ∏è 8,055 expert demonstrations (25,743 corresponding language direc-<br>tives)                                                                                           | üñºÔ∏è Resnet18 features, video sequence<br> üó£Ô∏è Language <br> ü§ñ Action                                                                                                      | University of Washington, Allen Institute for AI                                         | Trajectory JSON, Raw Images, MP4, PDDL | AI2-THOR agents                                                                                                              | Iteractive visual dataset with high-level goal and low-level<br>natural language instructions for object and environment interaction.                                               |
| VirtualHome                                                                     | üåê [Project Website](http://virtual-home.org/)üìÑ [Paper](http://virtual-home.org/paper/virtualhome.pdf)üíæ [Dataset Download](https://github.com/xavierpuigf/virtualhome/blob/master/virtualhome/dataset/README.md)                                                                                                                                                                          | 2018 | üß©2821 ActivityPrograms,  üß© 5193 SyntheticPrograms                                                                                                                       | üó£Ô∏è Semantic Segmentation, Natural Language Descriptions <br> üñºÔ∏è RGB-D data<br> ü§ñ Pose information                                                                       | MIT, University of Toronto                                                               | JSON                                   | Virtual avatars                                                                                                              | Embodied household benchmark, Activities in are represented by *programs* (sequence of actions) and _graphs_ (definition of the environment where the activity takes place).        |

---

## Real-world

| NAME            | Links                                                                                                                                                                                                                               | YEAR | SIZE                                                                    | MODALITIES                                                                                                                                                                                                 | UNIVERSITY / ORG                                                                                          | DATA FORMAT   | ROBOTS                                                                                       | NOTES                                                                                     |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------- | -------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| DROID           | üåê [Project Website](https://droid-dataset.github.io/) üìÑ [Paper](https://arxiv.org/abs/2403.12945) üíæ [Dataset Download](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing)                    | 2024 | üè† 564 scenes<br>üß© 86 tasks                                            | üñºÔ∏è full-HD stereo videos for all three cameras,<br>ü§ñ [RLDS](https://droid-dataset.github.io/droid/the-droid-dataset),<br> üó£Ô∏è [Language  annotation (added in 2025)](https://huggingface.co/KarlP/droid) | 13 Universities (Stanford, Berkeley, Princton, Washington and [others](https://droid-dataset.github.io/)) | RLDS          | Franka Panda                                                                                 | One hardware setup across 13 institutions for data collection.                            |
| BridgeData V2   | üåê [Project Website](https://rail-berkeley.github.io/bridgedata/)  üìÑ [Paper](https://arxiv.org/abs/2308.12952)  üíæ[Dataset Download](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/)                                 | 2023 | üè† 24 environments<br>üß© 13 skills                                      | üñºÔ∏è Multiple camera views and depth data<br>ü§ñ robot arm trajectory<br> üó£Ô∏è Natural language labels                                                                                                        | UC Berkeley, Stanford ,Google DeepMind, CMU                                                               | HDF5          | WidowX 250                                                                                   | Teleoperation of the robot with a VR<br>controller.                                       |
| Mobile ALOHA    | üåê [Project Website](https://mobile-aloha.github.io/) üìÑ [Paper](https://arxiv.org/abs/2401.02117)  üíæ [Dataset Download](https://drive.google.com/drive/folders/1FP5eakcxQrsHyiWBRDsMRvUfSxeykiDc)                                 | 2024 | üß© 7 tasks                                                              | üñºÔ∏è Three camera positions, cam_high, cam_left_wrist, cam_right_wrist <br>ü§ñ action, effort ,  pose <br>                                                                                                   | Stanford University                                                                                       | HDF5          | Mobile base + two ViperX 300 robotic arms. The teleoperation setup uses two WidowX 250 arms. | Bi-manual mobile manipulation. Co-training with existing static ALOHA datasets.           |
| Furniture Bench | üåê [Project Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) üìÑ [Paper](https://arxiv.org/abs/2305.12821) üíæ [Dataset Download](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) | 2023 | üß© 8 different furniture tasks<br>üïπÔ∏è 5100 teleoperation demonstrations | üñºÔ∏è  Wrist camera image, Front camera image <br>ü§ñ robot state                                                                                                                                             | Korea Advanced Institute of Science and Technology                                                        | Pickle format | Franka Panda                                                                                 | 3D printed furniture tasks. Data collected with Oculus Quest 2 controller and a keyboard. |
|                 |                                                                                                                                                                                                                                     |      |                                                                         |                                                                                                                                                                                                            |                                                                                                           |               |                                                                                              |                                                                                           |


---

## Combined (Sim + Real)

| NAME                                  | Links                                                                                                                                                                                                                                                                                                                                                                                                             | YEAR                                                                | SIZE                                                                                                                                                                                                                                                                  | MODALITIES                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | UNIVERSITY / ORG                                                                                                                                                                                                                                                         | DATA FORMAT                                                                   | ROBOTS                                                                                                                                                                                     | NOTES                                                                                                                                                                                                                                                                                                                                               |
| ------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Open-X Embodiment                     | üåê [Project Website](https://robotics-transformer-x.github.io/)  üìÑ [Paper](https://arxiv.org/abs/2310.08864)  üíæ[Dataset Download](https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb#scrollTo=N2Efw2aHVfSX)üìä[Dataset overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0) | 2023                                                                | dataset collection                                                                                                                                                                                                                                                    | üñºÔ∏è RGB, depth cameras  <br>üó£Ô∏è Language annotations<br>ü§ñ Proprioception                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Arizona State University,<br>Carnegie Mellon University ,Columbia University,<br>ETH Z√ºrich, Google DeepMind, and many [other](https://robotics-transformer-x.github.io/)<br><br>                                                                                        | RLDS                                                                          | Franka, Spot, Hello Stretch, UR5, Sawyer, ViperX, ...                                                                                                                                      | Information about exact modalities and robots in [google sheet overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0). To get SOTA results not all datasets are used (filtering is required). Open-X provides unified download and format of datasets.                                     |
| RoboTurk                              | üåê [Project Website](https://roboturk.stanford.edu/index.html) üìÑ [Paper](https://arxiv.org/abs/1811.02790)  üíæ [Dataset Download Real](https://roboturk.stanford.edu/dataset_real.html)<br>üíæ [Dataset Download Sim](https://roboturk.stanford.edu/dataset_sim.html)                                                                                                                                             | 2019                                                                | ***Real*** <br>üïπÔ∏è 2144 demonstrations<br>***Sim*** <br>üïπÔ∏è 1070 SawyerPickPlace<br>üïπÔ∏è1147 SawyerNutAssembly demonstrations                                                                                                                                          | ***Real***  <br>üñºÔ∏è Multiple camera views and depth data<br>ü§ñ control data from the user and joint data from the robot<br> ***Sim***<br> ü§ñ Scene and robot states                                                                                                                                                                                                                                                                                                                                                                                                | Stanford University                                                                                                                                                                                                                                                      | HDF5, MP4                                                                     | Sawyer                                                                                                                                                                                     | Real datasets contains laundry layout, tower creation, and object search tasks done (54 users)                                                                                                                                                                                                                                                      |
| RoboMimic                             | üåê [Project Website](https://robomimic.github.io/) üìÑ [Paper](https://arxiv.org/abs/2108.03298)  üíæ [Dataset Download](https://robomimic.github.io/docs/datasets/robomimic_v0.1.html)                                                                                                                                                                                                                             | 2021                                                                | üß© 5 tasks<br>üïπÔ∏è 500 human generated  trajectories<br>üïπÔ∏è5400 machine generated trajectories<br>                                                                                                                                                                     | ü§ñ Robot state, action, task observation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Stanford University, The University of Texas at Austin                                                                                                                                                                                                                   | HDF5                                                                          | Franka                                                                                                                                                                                     | Dataset contains very simple tasks Lift, PickandPlace. Data collected using the RoboTurk platfrom                                                                                                                                                                                                                                                   |
| <br><br><br><br><br><br><br><br>      |                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                     |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| MimicGen                              | üåê [Project Website](https://mimicgen.github.io/) üìÑ [Paper](https://arxiv.org/abs/2310.17596) üíæ [Dataset Download](https://huggingface.co/datasets/amandlek/mimicgen_datasets)                                                                                                                                                                                                                                  | 2023                                                                | üïπÔ∏è 48,000 task demonstrations<br>üß©12 tasks.                                                                                                                                                                                                                         | üñºÔ∏è Image.<br>ü§ñ Robot state, action, task observation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | The University of Texas at Austin                                                                                                                                                                                                                                        | HDF5                                                                          | Panda, Sawyer, IIWA, UR5e                                                                                                                                                                  | A system for automatically synthesizing large-scale datasets from a small number of human demonstrations by adapting them to new scene configurations, object instances, and robot arms.                                                                                                                                                            |
| RH20T                                 | üåê [Project Website](https://rh20t.github.io/) üìÑ [Paper](https://arxiv.org/abs/2307.00595) üíæ [Dataset Download](https://rh20t.github.io/#download)                                                                                                                                                                                                                                                              | 2023. [[8]](https://www.robot-learning.ml/2023/files/paper51.pdf)   | 110,000+ contact-rich robot manipulation sequences. [[8]](https://www.robot-learning.ml/2023/files/paper51.pdf)                                                                                                                                                       | üñºÔ∏è Visual (RGBD) <br>üîä Audio <br>ü¶æ Force <br>ü§ñ Action information. [[8]](https://www.robot-learning.ml/2023/files/paper51.pdf)<br><br>\|   \|   \|   \|<br>\|---\|---\|---\|<br>\|RGB image\|1280 x 720 x 3\|10 Hz\|<br>\|Depth image\|1280 x 720\|10 Hz\|<br>\|Binocular IR images\|2 x 1280 x 720\|10 Hz\|<br>\|Robot joint angle\|6 / 7\|10 Hz\|<br>\|Robot joint torque\|6 / 7\|10 Hz\|<br>\|Gripper Cartesian pose\|6 / 7\|100 Hz\|<br>\|Gripper width\|1\|10 Hz\|<br>\|6-DoF Force/Torque\|6\|100 Hz\|<br>\|Audio\|N/A\|30 Hz\|<br>\|Fingertip tactile\| | Shanghai AI Laboratory, Beihang University, Shanghai Jiao Tong University, University of Sydney, University of Electronic Science and Technology of China, Nanjing University of Posts and Telecommunications. [[9]](https://sites.google.com/view/rh20t-primitive/main) | tar.gz. [[10]](https://rh20t.github.io/)                                      | Multiple robot arms (4 popular types). [[11]](https://www.researchgate.net/publication/372074968_RH20T_A_Robotic_Dataset_for_Learning_Diverse_Skills_in_One-Shot)                          | ## `Human Demonstration, Robot Manipulation>` pairs for each task. Each sequence includes visual, force, audio, and action information, along with a corresponding human demonstration video and language description. [[8]](https://www.robot-learning.ml/2023/files/paper51.pdf)[[12]](https://rh20t.github.io/static/RH20T_paper_compressed.pdf) |
| MOMA gen                              | üåê [Project Website](https://sites.google.com/view/il-for-mm/home) üìÑ [Paper](https://arxiv.org/abs/2112.05251) üíæ [Dataset Download](https://robomimic.github.io/docs/datasets/momart.html)                                                                                                                                                                                                                      | 2022. [[13]](https://robomimic.github.io/docs/datasets/momart.html) | 1200+ successful demonstrations across five long-horizon tasks. [[14]](https://sites.google.com/view/il-for-mm/home)                                                                                                                                                  | üñºÔ∏è Multi-sensor modalities. [[14]](https://sites.google.com/view/il-for-mm/home)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Stanford University. [[13]](https://robomimic.github.io/docs/datasets/momart.html)                                                                                                                                                                                       | HDF5. [[13]](https://robomimic.github.io/docs/datasets/momart.html)           | Mobile manipulators (e.g., Fetch, PAL Tiago). [[14]](https://sites.google.com/view/il-for-mm/home)                                                                                         | Stands for Mobile Manipulation RoboTurk (MoMaRT). It is a collection of demonstrations on long-horizon robot mobile manipulation tasks in a realistic simulated kitchen setting. [[13]](https://robomimic.github.io/docs/datasets/momart.html)[[15]](https://arxiv.org/abs/2112.05251)                                                              |
| MOMA gen                              | üåê [Project Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) üìÑ [Paper](https://arxiv.org/abs/2305.12821) üíæ [Dataset Download](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)                                                                                                                                                                               | https://momagen-rss.github.io/                                      |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| Mobile Manipulation RoboTurk (MoMaRT) | üåê [Project Website](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html) üìÑ [Paper](https://arxiv.org/abs/2305.12821) üíæ [Dataset Download](https://clvrai.github.io/furniture-bench/docs/tutorials/dataset.html)                                                                                                                                                                               | https://sites.google.com/view/il-for-mm/home                        |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| Mobile Manipulation RoboTurk (MoMaRT) | üåê [Project Website](https://sites.google.com/view/il-for-mm/home) üìÑ [Paper](https://arxiv.org/abs/2112.05251) üíæ [Dataset Download](https://robomimic.github.io/docs/datasets/momart.html)                                                                                                                                                                                                                      | 2021. [[15]](https://arxiv.org/abs/2112.05251)                      | 1200+ successful demonstrations across 5 long-horizon tasks. [[14]](https://sites.google.com/view/il-for-mm/home)                                                                                                                                                     | üñºÔ∏è Multi-sensor modalities. [[14]](https://sites.google.com/view/il-for-mm/home)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Stanford University. [[15]](https://arxiv.org/abs/2112.05251)                                                                                                                                                                                                            | HDF5. [[13]](https://robomimic.github.io/docs/datasets/momart.html)           | Fetch, PAL Tiago. [[14]](https://sites.google.com/view/il-for-mm/home)                                                                                                                     | A novel teleoperation framework allowing simultaneous navigation and manipulation of mobile manipulators, and a large-scale dataset collected in a realistic simulated kitchen setting. [[15]](https://arxiv.org/abs/2112.05251)                                                                                                                    |
| D4RL                                  | üåê [Project Website](https://sites.google.com/view/d4rl-anonymous/) üìÑ [Paper](https://arxiv.org/abs/2004.07219) üíæ [Dataset Download](https://huggingface.co/datasets/imone/D4RL)                                                                                                                                                                                                                                | 2020. [[16]](https://arxiv.org/abs/2004.07219)                      | Datasets for various tasks including locomotion, traffic management, autonomous driving, and robotics. [[17]](https://scispace.com/pdf/d4rl-datasets-for-deep-data-driven-reinforcement-learning-162ppxtve5.pdf)[[18]](https://sites.google.com/view/d4rl-anonymous/) | ü§ñ Physical information (3D position, orientation, joint angles, etc.). [[19]](https://di-engine-docs.readthedocs.io/en/latest/13_envs/d4rl.html)                                                                                                                                                                                                                                                                                                                                                                                                                  | Google, UC Berkeley. [[20]](https://openreview.net/forum?id=px0-N3_KjA)                                                                                                                                                                                                  | RLDS format. [[21]](https://www.tensorflow.org/datasets/catalog/d4rl_antmaze) | Ant, Hopper, HalfCheetah, Walker2D, Adroit, Franka Kitchen. [[18]](https://sites.google.com/view/d4rl-anonymous/)[[19]](https://di-engine-docs.readthedocs.io/en/latest/13_envs/d4rl.html) | An open-source benchmark for offline reinforcement learning. It provides standardized environments and datasets for training and benchmarking algorithms. [[16]](https://arxiv.org/abs/2004.07219)[[19]](https://di-engine-docs.readthedocs.io/en/latest/13_envs/d4rl.html)                                                                         |
| # DexMimicGen                         |                                                                                                                                                                                                                                                                                                                                                                                                                   | https://dexmimicgen.github.io/                                      |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| DexMimicGen                           | üåê [Project Website] üìÑ [Paper] üíæ [Dataset Download]                                                                                                                                                                                                                                                                                                                                                             | 2025                                                                | üïπÔ∏è 21,000+ demonstrations <br> üß© 9 tasks                                                                                                                                                                                                                            | ü§ñ Robot states, actions, and task observations from simulation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | NVIDIA Research, UT Austin, UC San Diego                                                                                                                                                                                                                                 | HDF5                                                                          | Bimanual dexterous robots (e.g., Fourier GR1 with Inspire hands) in simulation (Robosuite, BiGym).                                                                                         | An automated data generation system that creates large-scale datasets for bimanual dexterous manipulation from a small number of human demonstrations, leveraging a Real2Sim2Real pipeline. [[1]](https://dexmimicgen.github.io/)[[2]](https://chatpaper.com/paper/72822)                                                                           |
| RoboNet                               |                                                                                                                                                                                                                                                                                                                                                                                                                   | https://www.robonet.wiki/                                           |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| MT-Opt                                |                                                                                                                                                                                                                                                                                                                                                                                                                   | https://karolhausman.github.io/mt-opt/                              |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| BC-Z                                  |                                                                                                                                                                                                                                                                                                                                                                                                                   | https://sites.google.com/view/bc-z/home                             |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| RoboSet                               |                                                                                                                                                                                                                                                                                                                                                                                                                   | https://robopen.github.io/roboset/                                  |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |
| Dobee                                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | https://dobb-e.com/                                                 |                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                          |                                                                               |                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                     |


## Dataset Collections

| NAME              | Links                                                                                                                                                                                                                                                                                                                                                                                                             | YEAR | SIZE                                 | MODALITIES                                                                | UNIVERSITY / ORG                                                                                                                                                                  | DATA FORMAT | ROBOTS                                                | NOTES                                                                                                                                                                                                                                                                                                           |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------------------------------ | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ----------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Open-X Embodiment | üåê [Project Website](https://robotics-transformer-x.github.io/)  üìÑ [Paper](https://arxiv.org/abs/2310.08864)  üíæ[Dataset Download](https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb#scrollTo=N2Efw2aHVfSX)üìä[Dataset overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0) | 2023 | 71 datasets (last checked 15.9.2025) | üñºÔ∏è RGB, depth cameras  <br>üó£Ô∏è Language annotations<br>ü§ñ Proprioception | Arizona State University,<br>Carnegie Mellon University ,Columbia University,<br>ETH Z√ºrich, Google DeepMind, and many [other](https://robotics-transformer-x.github.io/)<br><br> | RLDS        | Franka, Spot, Hello Stretch, UR5, Sawyer, ViperX, ... | Information about exact modalities and robots in [google sheet overview](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0). To get SOTA results not all datasets are used (filtering is required). Open-X provides unified download and format of datasets. |



---
## Benchmarks

| NAME               | YEAR | SIM/REAL | TASK TYPE                                | SIMULATOR / PLATFORM | METRICS                                                                                                  | UNIVERSITY / ORG            | ROBOTS                           | OTHER NOTES                                                    |
| ------------------ | ---- | -------- | ---------------------------------------- | -------------------- | -------------------------------------------------------------------------------------------------------- | --------------------------- | -------------------------------- | -------------------------------------------------------------- |
| **DROID**          | 2024 | Real     | üñºÔ∏è Vision, ü§ñ Action                    | Real robots          | Success rate, Task diversity                                                                             | Google DeepMind             | Various (in-the-wild)            | Large-scale real-world dataset                                 |
| **LIBERO**         | 2023 | Sim      | üñºÔ∏è Vision, üó£Ô∏è Language, ü§ñ Action      | RoboSuite            | FWT (forward transfer), NBT (negative backward transfer), and<br>AUC (area under the success rate curve) | UT Austin, Tsinghua Univers | Franka                           | Lifelong learning in decision making and rocedurally generated |
| **BridgeData V2**  | 2023 | Real     | üñºÔ∏è Vision, üó£Ô∏è Language, ü§ñ Action      | Real robots + Sim    | Success rate, Task coverage                                                                              | UC Berkeley                 | Franka, UR5, others              | Used in RT-X benchmark                                         |
| **ALOHA**          | 2023 | Real     | üñºÔ∏è Vision, üõ†Ô∏è Force/Tactile, ü§ñ Action | Real robots (Franka) | Task success, Contact accuracy                                                                           | Stanford                    | Low-cost dual-arm robot (Franka) | Bimanual manipulation benchmark                                |
| **FurnitureBench** | 2023 | Real     | üñºÔ∏è Vision, ü§ñ Action                    | Real Franka robot    | Assembly success, Completion time                                                                        | KAIST                       | Franka Panda                     | Long-horizon assembly tasks                                    |
| **BEHAVIOR-1K**    | 2022 | Sim+Real | üñºÔ∏è Vision, ü§ñ Action                    | iGibson 2.0 + Robots | Task success, Diversity coverage                                                                         | Stanford                    | Franka, Fetch, TurtleBot         | 1,000 household activities benchmark                           |
| **CALVIN**         | 2022 | Real     | üñºÔ∏è Vision, üó£Ô∏è Language, ü§ñ Action      | Real Franka robots   | Success rate, Language grounding                                                                         | TU Dresden                  | Franka Panda                     | Language-conditioned manipulation                              |
| **RT-1**           | 2022 | Real     | üñºÔ∏è Vision, ü§ñ Action                    | Real wheeled arms    | Success rate, Generalization                                                                             | Google Robotics             | Everyday Robots (Wheeled arms)   | Transformer-based policy benchmark                             |
| **Habitat 2.0**    | 2021 | Sim+Real | üñºÔ∏è Vision, üó£Ô∏è Language, ü§ñ Action      | Habitat Sim          | Navigation success, Rearrangement metrics                                                                | FAIR, Georgia Tech          | LoCoBot, Fetch                   | Rearrangement + embodied AI tasks                              |
| **RoboMimic**      | 2021 | Sim      | üñºÔ∏è Vision, üìê State, ü§ñ Action          | RoboSuite (MuJoCo)   | Policy success, Offline RL metrics                                                                       | UC Berkeley                 | Sawyer, Franka (sim)             | Benchmark for offline imitation learning                       |
| **iGibson 2.0**    | 2021 | Sim      | üñºÔ∏è Vision, üìê State, ü§ñ Action          | iGibson 2.0          | Navigation success, Coverage                                                                             | Stanford                    | LoCoBot, Fetch (sim)             | Object-centric embodied AI benchmark                           |
| **ALFRED**         | 2020 | Sim      | üñºÔ∏è Vision, üó£Ô∏è Language, ü§ñ Action      | AI2-THOR             | Goal-condition success (GC-SR), Path length                                                              | Allen Institute for AI      | Virtual agents (AI2-THOR)        | Instruction-following tasks                                    |
| **RLBench**        | 2020 | Sim      | üñºÔ∏è Vision, üó£Ô∏è Language, ü§ñ Action      | PyRep (MuJoCo-like)  | Task success, Generalization                                                                             | Imperial College London     | Franka Panda (sim)               | Multi-task manipulation benchmark                              |
| **BEHAVIOR-100**   | 2020 | Sim+Real | üñºÔ∏è Vision, ü§ñ Action                    | iGibson              | Task success, Diversity coverage                                                                         | Stanford                    | Fetch, LoCoBot                   | Subset of BEHAVIOR-1K                                          |
| **Meta-World**     | 2019 | Sim      | üìê State, ü§ñ Action                      | MuJoCo               | Success rate, Generalization                                                                             | UC Berkeley                 | Sawyer (sim)                     | Multi-task & meta-RL tasks                                     |
| **VirtualHome**    | 2018 | Sim      | üó£Ô∏è Language, üñºÔ∏è Vision, ü§ñ Action      | Unity3D              | Script success, Task diversity                                                                           | MIT, Stanford               | Virtual agents (avatars)         | Household activity simulation benchmark                        |






---
# Physics-Based Simulation Frameworks

| Framework            | Associated Benchmarks & Environments                                                                                                               |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **NVIDIA Isaac Sim** | BEHAVIOR-1K, ARNOLD, Isaac Lab, Isaac Lab Mimic                                                                                                    |
| **MuJoCo**           | Robosuite (incl. Robomimic, LIBERO, MuBle, CompoSuite, RoboCasa), Meta-World (incl. Continual World), MuJoBan, ALOHA, VLABench, Gymnasium-Robotics |
| **SAPIEN**           | ManiSkill 2, SimplerEnv, ClevrSkills, VLATest                                                                                                      |
| **Unity**            | AI2-THOR (incl. ManipulaTHOR, ProcTHOR, ALFRED)                                                                                                    |
| **PyBullet**         | CausalWorld, Habitat-Sim, Calvin                                                                                                                   |
| **CoppeliaSim**      | PyRep, RLBench (incl. AGNOSTOS)                                                                                                                    |
| **Webots**           | deepbots                                                                                                                                           |
| **Drake**            |                                                                                                                                                    |
| Genesis              |                                                                                                                                                    |
|                      |                                                                                                                                                    |


---

### Text-Oriented

| Framework / Type  | Associated Benchmarks & Datasets |
| ----------------- | -------------------------------- |
| **Text-Oriented** |                                  |

### Real world Framework
| Manipulation Type | Associated Benchmarks & Datasets |
| ----------------- | -------------------------------- |
| Bimanual          | ALOHA ALOHA 2                    |
| Unimanual         |                                  |
| Mobile            |                                  |

---

Models and Agents

https://robopen.github.io/
https://octo-models.github.io/

